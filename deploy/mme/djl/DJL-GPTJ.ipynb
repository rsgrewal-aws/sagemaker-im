{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dab6d19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting boto3==1.24.68\n",
      "  Downloading boto3-1.24.68-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: botocore<1.28.0,>=1.27.68 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3==1.24.68) (1.27.84)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3==1.24.68) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3==1.24.68) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.68->boto3==1.24.68) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.68->boto3==1.24.68) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.68->boto3==1.24.68) (1.16.0)\n",
      "Installing collected packages: boto3\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.24.79\n",
      "    Uninstalling boto3-1.24.79:\n",
      "      Successfully uninstalled boto3-1.24.79\n",
      "Successfully installed boto3-1.24.68\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install boto3==1.24.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac0014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.0-deepspeed: Pulling from deepjavalibrary/djl-serving\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "602a45a9c0c5: Pulling fs layer\n",
      "e1bae4c1f40f: Pulling fs layer\n",
      "d9d586ab2510: Pulling fs layer\n",
      "2b44adc78060: Pulling fs layer\n",
      "cd4d84563a60: Pulling fs layer\n",
      "e19a4e23074d: Pulling fs layer\n",
      "a69bd65705b8: Pulling fs layer\n",
      "7145f7b4815b: Pulling fs layer\n",
      "e367c0f08642: Pulling fs layer\n",
      "eee5fed2d5ca: Pulling fs layer\n",
      "46b22735db66: Pulling fs layer\n",
      "dc2cc42a02f3: Pulling fs layer\n",
      "41ae858b91d1: Pulling fs layer\n",
      "d46f0903e5a5: Pulling fs layer\n",
      "3cb1e2965cce: Pulling fs layer\n",
      "0391b9b1de9f: Pulling fs layer\n",
      "d4d8280fcfac: Pulling fs layer\n",
      "eee5fed2d5ca: Waiting\n",
      "46b22735db66: Waiting\n",
      "dc2cc42a02f3: Waiting\n",
      "41ae858b91d1: Waiting\n",
      "d46f0903e5a5: Waiting\n",
      "3cb1e2965cce: Waiting\n",
      "0391b9b1de9f: Waiting\n",
      "d4d8280fcfac: Waiting\n",
      "d9d586ab2510: Waiting\n",
      "2b44adc78060: Waiting\n",
      "cd4d84563a60: Waiting\n",
      "e19a4e23074d: Waiting\n",
      "a69bd65705b8: Waiting\n",
      "7145f7b4815b: Waiting\n",
      "e367c0f08642: Waiting\n",
      "602a45a9c0c5: Verifying Checksum\n",
      "602a45a9c0c5: Download complete\n",
      "e1bae4c1f40f: Verifying Checksum\n",
      "e1bae4c1f40f: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "2b44adc78060: Verifying Checksum\n",
      "2b44adc78060: Download complete\n",
      "d9d586ab2510: Verifying Checksum\n",
      "d9d586ab2510: Download complete\n",
      "e19a4e23074d: Download complete\n",
      "7145f7b4815b: Verifying Checksum\n",
      "7145f7b4815b: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "602a45a9c0c5: Pull complete\n",
      "e1bae4c1f40f: Pull complete\n",
      "d9d586ab2510: Pull complete\n",
      "2b44adc78060: Pull complete\n",
      "cd4d84563a60: Verifying Checksum\n",
      "cd4d84563a60: Download complete\n",
      "eee5fed2d5ca: Verifying Checksum\n",
      "eee5fed2d5ca: Download complete\n",
      "46b22735db66: Verifying Checksum\n",
      "46b22735db66: Download complete\n",
      "dc2cc42a02f3: Verifying Checksum\n",
      "dc2cc42a02f3: Download complete\n",
      "41ae858b91d1: Verifying Checksum\n",
      "41ae858b91d1: Download complete\n",
      "d46f0903e5a5: Download complete\n",
      "3cb1e2965cce: Verifying Checksum\n",
      "3cb1e2965cce: Download complete\n",
      "a69bd65705b8: Verifying Checksum\n",
      "a69bd65705b8: Download complete\n",
      "0391b9b1de9f: Verifying Checksum\n",
      "0391b9b1de9f: Download complete\n",
      "e367c0f08642: Verifying Checksum\n",
      "e367c0f08642: Download complete\n",
      "d4d8280fcfac: Verifying Checksum\n",
      "d4d8280fcfac: Download complete\n",
      "cd4d84563a60: Pull complete\n",
      "e19a4e23074d: Pull complete\n",
      "a69bd65705b8: Pull complete\n",
      "7145f7b4815b: Pull complete\n",
      "e367c0f08642: Pull complete\n",
      "eee5fed2d5ca: Pull complete\n",
      "46b22735db66: Pull complete\n",
      "dc2cc42a02f3: Pull complete\n",
      "41ae858b91d1: Pull complete\n",
      "d46f0903e5a5: Pull complete\n",
      "3cb1e2965cce: Pull complete\n",
      "0391b9b1de9f: Pull complete\n",
      "d4d8280fcfac: Pull complete\n",
      "Digest: sha256:41848dffa70483c3af8b2420135c0e0d6b5e32c6cd21d2cef94e10b0bae6fe47\n",
      "Status: Downloaded newer image for deepjavalibrary/djl-serving:0.18.0-deepspeed\n",
      "docker.io/deepjavalibrary/djl-serving:0.18.0-deepspeed\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "docker pull deepjavalibrary/djl-serving:0.18.0-deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41815aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                                   TAG                IMAGE ID       CREATED        SIZE\n",
      "catboost-sagemaker-multimodel                                                latest             befea775c8ef   12 hours ago   1.35GB\n",
      "874199810560.dkr.ecr.us-east-1.amazonaws.com/catboost-sagemaker-multimodel   latest             befea775c8ef   12 hours ago   1.35GB\n",
      "ubuntu                                                                       18.04              35b3f4f76a24   3 weeks ago    63.1MB\n",
      "deepjavalibrary/djl-serving                                                  0.18.0-deepspeed   c2edba9c6d73   2 months ago   12.7GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b94ece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [874199810560.dkr.ecr.us-east-1.amazonaws.com/djl_deepspeed]\n",
      "42737ae997a9: Preparing\n",
      "4e257e38b819: Preparing\n",
      "b16fc1914e14: Preparing\n",
      "7e20e12fff52: Preparing\n",
      "01b6a2323efe: Preparing\n",
      "55df8d287560: Preparing\n",
      "61d781c1452a: Preparing\n",
      "dbc3bf935e02: Preparing\n",
      "1a5fac543081: Preparing\n",
      "a8d0c4c62eef: Preparing\n",
      "7ed9a71261c7: Preparing\n",
      "a1eeba43cdbe: Preparing\n",
      "6127942867a5: Preparing\n",
      "e592fe6d10a9: Preparing\n",
      "f42691182163: Preparing\n",
      "68016c5bb65c: Preparing\n",
      "8034550a3bbe: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "7ed9a71261c7: Waiting\n",
      "a1eeba43cdbe: Waiting\n",
      "6127942867a5: Waiting\n",
      "e592fe6d10a9: Waiting\n",
      "f42691182163: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "8034550a3bbe: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "61d781c1452a: Waiting\n",
      "dbc3bf935e02: Waiting\n",
      "1a5fac543081: Waiting\n",
      "a8d0c4c62eef: Waiting\n",
      "55df8d287560: Waiting\n",
      "01b6a2323efe: Pushed\n",
      "7e20e12fff52: Pushed\n",
      "b16fc1914e14: Pushed\n",
      "61d781c1452a: Pushed\n",
      "55df8d287560: Pushed\n",
      "dbc3bf935e02: Pushed\n",
      "a8d0c4c62eef: Pushed\n",
      "a1eeba43cdbe: Pushed\n",
      "4e257e38b819: Pushed\n",
      "e592fe6d10a9: Pushed\n",
      "f42691182163: Pushed\n",
      "68016c5bb65c: Pushed\n",
      "8034550a3bbe: Pushed\n",
      "bf8cedc62fb3: Pushed\n",
      "6127942867a5: Pushed\n",
      "7ed9a71261c7: Pushed\n",
      "1a5fac543081: Pushed\n",
      "42737ae997a9: Pushed\n",
      "latest: digest: sha256:41848dffa70483c3af8b2420135c0e0d6b5e32c6cd21d2cef94e10b0bae6fe47 size: 4098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our container\n",
    "img=djl_deepspeed\n",
    "\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${img}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${img}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${img}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "\n",
    "# # Build the docker image locally with the image name and then push it to ECR\n",
    "docker tag deepjavalibrary/djl-serving:0.18.0-deepspeed ${fullname}\n",
    "\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c91a5",
   "metadata": {},
   "source": [
    "#### Write Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "774ff365",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p djl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import deepspeed\n",
    "from djl_python import Input, Output\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import is_offline_mode\n",
    "\n",
    "\n",
    "### Model loading and instantiating on GPUs\n",
    "def get_repo_root(model_name_or_path, revision=None):\n",
    "    # checks if online or not\n",
    "    if is_offline_mode():\n",
    "\n",
    "        logging.info(\"Offline mode: forcing local_files_only=True\")\n",
    "        local_files_only = True\n",
    "    else:\n",
    "        local_files_only = False\n",
    "\n",
    "    # loads files from hub\n",
    "    cached_repo_dir = snapshot_download(\n",
    "        model_name_or_path, allow_patterns=[\"*\"], local_files_only=local_files_only, revision=revision\n",
    "    )\n",
    "\n",
    "    return cached_repo_dir\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    tensor_parallel = int(os.getenv('TENSOR_PARALLEL_DEGREE', '1'))\n",
    "    model_name = \"microsoft/bloom-deepspeed-inference-int8\"\n",
    "    deepspeed.init_distributed(\"nccl\")\n",
    "    logging.info(f\"Loading the model {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    # Construct model with fake meta tensors, later will be replaced during ds-inference ckpt load\n",
    "    with deepspeed.OnDevice(dtype=torch.float16, device=\"meta\"):\n",
    "        model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16)\n",
    "    model = model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### Deepspeed-Inference Loading\n",
    "    repo_root = get_repo_root(model_name)\n",
    "    # tp presharded repos come with their own checkpoints config file\n",
    "    checkpoints_json = os.path.join(repo_root, \"ds_inference_config.json\")\n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        base_dir=repo_root,\n",
    "        dtype=torch.int8,\n",
    "        checkpoint=checkpoints_json,\n",
    "        replace_with_kernel_inject=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    deepspeed.runtime.utils.see_memory_usage(\"post-ds-inference-init\", force=True)\n",
    "    model = model.module\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def batch_generation(batch_size):\n",
    "    input_sentences = [\n",
    "        \"DeepSpeed is a machine learning framework\",\n",
    "        \"He is working on\",\n",
    "        \"He has a\",\n",
    "        \"He got all\",\n",
    "        \"Everyone is happy and I can\",\n",
    "        \"The new movie that got Oscar this year\",\n",
    "        \"In the far far distance from our galaxy,\",\n",
    "        \"Peace is the only way\",\n",
    "    ]\n",
    "    if batch_size > len(input_sentences):\n",
    "        # dynamically extend to support larger bs by repetition\n",
    "        input_sentences *= math.ceil(batch_size / len(input_sentences))\n",
    "    return input_sentences[: batch_size]\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model()\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    batch_size = data[\"batch_size\"]\n",
    "    tokens_to_gen = data[\"text_length\"]\n",
    "    generate_kwargs = dict(min_length=tokens_to_gen, max_new_tokens=tokens_to_gen, do_sample=False)\n",
    "    input_tokens = tokenizer.batch_encode_plus(batch_generation(batch_size), return_tensors=\"pt\", padding=True)\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to(torch.cuda.current_device())\n",
    "    outputs = model.generate(**input_tokens, **generate_kwargs)\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return Output().add_as_json(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63111bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting djl/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile djl/model.py\n",
    "\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def _get_model_bloom_176():\n",
    "    model_name = \"EleutherAI/gpt-j-6B\"\n",
    "    tensor_parallel = int(os.getenv(\"TENSOR_PARALLEL_DEGREE\", \"2\"))\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    print(\"Bloom:176:LLM:Tokenizer is initialized\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True, \n",
    "        max_memory={\n",
    "            0: \"0GIB\",\n",
    "            1: \"26GIB\",\n",
    "            2: \"26GIB\",\n",
    "            3: \"26GIB\",\n",
    "            4: \"26GIB\",\n",
    "            5: \"26GIB\",\n",
    "            6: \"26GIB\",\n",
    "            7: \"26GIB\",\n",
    "        } \n",
    "    )\n",
    "    #model.requires_grad_(False)\n",
    "    #model.eval()\n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=model.dtype,\n",
    "        replace_method=\"auto\",\n",
    "        replace_with_kernel_inject=True,\n",
    "    )\n",
    "    generator = pipeline(\n",
    "        task=\"text-generation\", model=model, tokenizer=tokenizer, device=local_rank\n",
    "    )\n",
    "    return generator\n",
    "    \n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model_name = \"EleutherAI/gpt-j-6B\"\n",
    "    tensor_parallel = int(os.getenv(\"TENSOR_PARALLEL_DEGREE\", \"2\"))\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, revision=\"float32\", torch_dtype=torch.float32\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=model.dtype,\n",
    "        replace_method=\"auto\",\n",
    "        replace_with_kernel_inject=True,\n",
    "    )\n",
    "    generator = pipeline(\n",
    "        task=\"text-generation\", model=model, tokenizer=tokenizer, device=local_rank\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model()\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_string()\n",
    "    result = predictor(data, do_sample=True, min_tokens=200, max_new_tokens=256)\n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf615955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting djl/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile djl/serving.properties\n",
    "engine = Rubikon\n",
    "gpu.minWorkers=1\n",
    "gpu.maxWorkers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f685a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p djl/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9c3a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting djl/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile djl/code/inference.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "def download_files(files, input_path):\n",
    "    \n",
    "    def _download_file(s3_path, input_path):\n",
    "        \n",
    "        global s3_client\n",
    "        \n",
    "        local_file_path = os.path.join(input_path, s3_path.split(\"/\")[-1])\n",
    "        \n",
    "        bucket, *key = s3_path.split(\"/\")\n",
    "        key = \"/\".join(key)\n",
    "        \n",
    "        try:\n",
    "            s3_client.download_file(bucket, key, local_file_path)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            s3_client.download_file(bucket, key, local_file_path)\n",
    "\n",
    "        return local_file_path\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(_download_file, file, input_path) for file in files]\n",
    "        for future in as_completed(futures):\n",
    "            print (f\"Bloom:176:LLM:downloaded: {future.result()}\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    bucket = os.environ.get(\"MODEL_S3_BUCKET\")\n",
    "    key_prefix = os.environ.get(\"MODEL_S3_PREFIX\")\n",
    "    print(f\"Bloom:176:LLM:bucket={bucket}::key={key_prefix}\")\n",
    "    model_dir = \"/tmp/model\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    s3_objects = s3_client.list_objects(Bucket=bucket, Prefix=key_prefix)[\"Contents\"]\n",
    "    s3_paths = [os.path.join(bucket, obj[\"Key\"]) for obj in s3_objects]\n",
    "    print(\"Bloom:176:LLM:downloading files\")\n",
    "    download_files(s3_paths, model_dir)\n",
    "    print(\"Bloom:176:LLM:downloading finished\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    print(\"Bloom:176:LLM:Tokenizer is initialized\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True, \n",
    "        max_memory={\n",
    "            0: \"0GIB\",\n",
    "            1: \"26GIB\",\n",
    "            2: \"26GIB\",\n",
    "            3: \"26GIB\",\n",
    "            4: \"26GIB\",\n",
    "            5: \"26GIB\",\n",
    "            6: \"26GIB\",\n",
    "            7: \"26GIB\",\n",
    "        } \n",
    "    )\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    print(\"Bloom:176:LLM:Loaded the model\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    print(\"Bloom:176:LLM:predict_fn request received\")\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    print(\"Bloom:176:LLM:Input text is \"+ text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    result_length = 50 \n",
    "    #output_sequences = model.generate(input_ids=encoded_input['input_ids'], **data)\n",
    "    output_sequences = model.generate( input_ids=encoded_input[\"input_ids\"], max_length=result_length)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "#Override out of the box input function\n",
    "def input_fn(input_data, content_type):\n",
    "    print(\"Bloom:176:LLM:Received the input \" + input_data)\n",
    "    print(\"Bloom:176:LLM:Content type \" + content_type)\n",
    "    if content_type == \"application/json\":\n",
    "        return json.loads(input_data)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "#Override out of the box output function\n",
    "def output_fn(prediction, accept):\n",
    "    print(\"Bloom:176:LLM:Returning the output \" + prediction)\n",
    "    print(\"Bloom:176:LLM:accept type \" + accept)\n",
    "    output = {\"outputs\": prediction}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d1664a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "account = session.account_id()\n",
    "region = session.boto_region_name\n",
    "img = \"djl_deepspeed\"\n",
    "fullname = account + \".dkr.ecr.\" + region + \".amazonaws.com/\" + img + \":latest\"\n",
    "bucket = session.default_bucket()\n",
    "path = \"s3://\" + bucket + \"/DEMO-djl-big-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a851d7f",
   "metadata": {},
   "source": [
    "#### Tar structure"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77ea8bc6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "model.tar.gz/\n",
    "\n",
    "|- djl\n",
    "|- model.py\n",
    "|- serving.properties\n",
    "|- code/\n",
    "  |- inference.py\n",
    "  |- requirements.txt \n",
    "  \n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. \n",
    "    Customers using this would have to implement preprocess, predict and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f71229b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djl/\n",
      "djl/model.py\n",
      "djl/code/\n",
      "djl/code/inference.py\n",
      "djl/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "rm gpt-j.tar.gz\n",
    "#always start fresh\n",
    "\n",
    "#mkdir -p gpt-j\n",
    "#mv model.py gpt-j\n",
    "#mv serving.properties gpt-j\n",
    "tar -czvf gpt-j.tar.gz djl/\n",
    "#aws s3 cp gpt-j.tar.gz {path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e63dea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-874199810560/DEMO-djl-big-model/gpt-j.tar.gz'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s3_url = sagemaker.s3.S3Uploader.upload(\n",
    "    \"gpt-j.tar.gz\", path, kms_key=None, sagemaker_session=session\n",
    ")\n",
    "model_s3_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bf3a9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelArn': 'arn:aws:sagemaker:us-east-1:874199810560:model/gpt-j-2022-10-03-15-38-57',\n",
       " 'ResponseMetadata': {'RequestId': 'f38121b3-8a30-4129-9432-51f337f18f2d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f38121b3-8a30-4129-9432-51f337f18f2d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '87',\n",
       "   'date': 'Mon, 03 Oct 2022 15:38:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "time_stamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_name = \"gpt-j-\" + time_stamp\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=session.get_caller_identity_arn(),\n",
    "    PrimaryContainer={\n",
    "        \"Image\": fullname,\n",
    "        \"ModelDataUrl\": model_s3_url,\n",
    "        \"Environment\": {\"TENSOR_PARALLEL_DEGREE\": \"2\"},\n",
    "    },\n",
    ")\n",
    "create_model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4f662",
   "metadata": {},
   "source": [
    "Now we create an endpoint configuration that SageMaker hosting services uses to deploy models. Note that we configured ModelDataDownloadTimeoutInSeconds and ContainerStartupHealthCheckTimeoutInSeconds to accommodate the large size of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d79bf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:874199810560:endpoint-config/t-j-config-2022-10-03-15-38-57',\n",
       " 'ResponseMetadata': {'RequestId': 'ca76b43d-b686-4d21-aa11-fc1146290fd0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ca76b43d-b686-4d21-aa11-fc1146290fd0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '111',\n",
       "   'date': 'Mon, 03 Oct 2022 15:39:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_instance_count = 1\n",
    "instance_type = \"ml.p3.2xlarge\" # \"ml.g5.48xlarge\"\n",
    "variant_name = \"AllTraffic\"\n",
    "endpoint_config_name = \"t-j-config-\" + time_stamp\n",
    "\n",
    "production_variants = [\n",
    "    {\n",
    "        \"VariantName\": variant_name,\n",
    "        \"ModelName\": model_name,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"ModelDataDownloadTimeoutInSeconds\": 1800,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config = {\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "    \"ProductionVariants\": production_variants,\n",
    "}\n",
    "\n",
    "ep_conf_res = sm_client.create_endpoint_config(**endpoint_config)\n",
    "ep_conf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62c467d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:874199810560:endpoint/gpt-j2022-10-03-15-38-57',\n",
       " 'ResponseMetadata': {'RequestId': 'b3349fc1-f544-432d-b637-493d115aeabc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b3349fc1-f544-432d-b637-493d115aeabc',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '92',\n",
       "   'date': 'Mon, 03 Oct 2022 15:39:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"gpt-j\" + time_stamp\n",
    "ep_res = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "ep_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b28443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for gpt-j2022-10-03-15-38-57 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting for {} endpoint to be in service...\".format(endpoint_name))\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "print(\"Created {} endpoint is in Service and read to invoke ...\".format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "content_type = \"text/plain\"  # The MIME type of the input data in the request body.\n",
    "payload = \"Amazon.com is the best\"  # Payload for inference.\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=content_type, Body=payload\n",
    ")\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39205313",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf12cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
