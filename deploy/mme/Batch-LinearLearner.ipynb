{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker BATCH Transform examples\n",
    "With [Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html), Use batch transform when you need to do the following:\n",
    "\n",
    "    Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.\n",
    "\n",
    "    Get inferences from large datasets.\n",
    "\n",
    "    Run inference when you don't need a persistent endpoint.\n",
    "\n",
    "    Associate input records with inferences to assist the interpretation of results.\n",
    "\n",
    "To filter input data before performing inferences or to associate input records with inferences about those records, see Associate Prediction Results with Input Records. For example, you can filter input data to provide context for creating and interpreting reports about the output data.For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "To split input files into mini-batches when you create a batch transform job, set the SplitType parameter value to Line. If SplitType is set to None or if an input file can't be split into mini-batches, SageMaker uses the entire input file in a single request. Note that Batch Transform doesn't support CSV-formatted input that contains embedded newline characters. You can control the size of the mini-batches by using the BatchStrategy and MaxPayloadInMB parameters. MaxPayloadInMB must not be greater than 100 MB. If you specify the optional MaxConcurrentTransforms parameter, then the value of (MaxConcurrentTransforms * MaxPayloadInMB) must also not exceed 100 MB.\n",
    "\n",
    "If the batch transform job successfully processes all of the records in an input file, it creates an output file with the same name and the .out file extension. For multiple input files, such as input1.csv and input2.csv, the output files are named input1.csv.out and input2.csv.out. The batch transform job stores the output files in the specified location in Amazon S3, such as s3://awsexamplebucket/output/.\n",
    "\n",
    "\n",
    "\n",
    "![](./cw_charts/BatchTransform.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [TRANSFORM the raw housing data using Scikit Learn model](#Preprocess-synthetic-housing-data-using-scikit-learn)\n",
    "1. [Clean up](#CleanUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 1 - Generate synthetic data for housing models <a id='Generate-synthetic-data-for-housing-models'></a>\n",
    "\n",
    "In this section, you will generate synthetic data that will be used to train the linear learner models.  The data generated consists of 6 numerical features - the year the house was built in, house size in square feet, number of bedrooms, number of bathroom, the lot size and number of garages and two categorial features - deck and front_porch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from random import choice\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker.multidatamodel import MULTI_MODEL_CONTAINER_MODE\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET :  sagemaker-us-east-1-333262794411\n",
      "ROLE :  arn:aws:iam::333262794411:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-9N675HOUZIM5\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "BUCKET  = sagemaker_session.default_bucket()\n",
    "print(\"BUCKET : \", BUCKET)\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"ROLE : \", role)\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION = boto3.Session().region_name\n",
    "\n",
    "DATA_PREFIX = 'DEMO_MME_LINEAR_LEARNER'\n",
    "HOUSING_MODEL_NAME = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    \"\"\"Generate price based on features of the house\"\"\"\n",
    "    \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        garage = 1\n",
    "    else:\n",
    "        garage = 0\n",
    "        \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        front_porch = 1\n",
    "    else:\n",
    "        front_porch = 0\n",
    "        \n",
    "    price = int(150 * house['SQUARE_FEET'] + \\\n",
    "                10000 * house['NUM_BEDROOMS'] + \\\n",
    "                15000 * house['NUM_BATHROOMS'] + \\\n",
    "                15000 * house['LOT_ACRES'] + \\\n",
    "                10000 * garage + \\\n",
    "                10000 * front_porch + \\\n",
    "                15000 * house['GARAGE_SPACES'] - \\\n",
    "                5000 * (MAX_YEAR - house['YEAR_BUILT']))\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_yes_no():\n",
    "    \"\"\"Generate values (y/n) for categorical features\"\"\"\n",
    "    answer = choice(['y', 'n'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    \"\"\"Generate a row of data (single house information)\"\"\"\n",
    "    house = {'SQUARE_FEET':    np.random.normal(3000, 750),\n",
    "             'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "             'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "             'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "             'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "             'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10))),\n",
    "             'FRONT_PORCH':   gen_yes_no(),\n",
    "             'DECK':          gen_yes_no()\n",
    "            }\n",
    "    \n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [house['YEAR_BUILT'],   \n",
    "            house['SQUARE_FEET'], \n",
    "            house['NUM_BEDROOMS'], \n",
    "            house['NUM_BATHROOMS'], \n",
    "            house['LOT_ACRES'],    \n",
    "            house['GARAGE_SPACES'],\n",
    "            house['FRONT_PORCH'],    \n",
    "            house['DECK'], \n",
    "            price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    \"\"\"Generate housing dataset\"\"\"\n",
    "    house_list = []\n",
    "    \n",
    "    for _ in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        house_list, \n",
    "        columns=[\n",
    "            'YEAR_BUILT',    \n",
    "            'SQUARE_FEET',  \n",
    "            'NUM_BEDROOMS',            \n",
    "            'NUM_BATHROOMS',\n",
    "            'LOT_ACRES',\n",
    "            'GARAGE_SPACES',\n",
    "            'FRONT_PORCH',\n",
    "            'DECK', \n",
    "            'PRICE']\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, test): \n",
    "    \"\"\"Save the housing data locally\"\"\"\n",
    "    os.makedirs('data/{0}/train'.format(location), exist_ok=True)\n",
    "    train.to_csv('data/{0}/train/train.csv'.format(location), sep=',', header=False, index=False)\n",
    "       \n",
    "    os.makedirs('data/{0}/test'.format(location), exist_ok=True)\n",
    "    test.to_csv('data/{0}/test/test.csv'.format(location), sep=',', header=False, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate housing data for multiple locations.\n",
    "#Change \"PARALLEL_TRAINING_JOBS \" to a lower number to limit the number of training jobs and models. Or to a higher value to experiment with more models.\n",
    "\n",
    "PARALLEL_TRAINING_JOBS = 1\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    \n",
    "    #Spliting data into train and test in 90:10 ratio\n",
    "    #Not splitting the train data into train and val because its not preprocessed yet\n",
    "    train, test = train_test_split(houses, test_size=0.1)\n",
    "    save_data_locally(loc, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the first few lines of data.\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Preprocess the raw housing data using Scikit Learn <a id='Preprocess-synthetic-housing-data-using-scikit-learn'></a>\n",
    "\n",
    "In this section, the categorical features of the data (deck and porch) are pre-processed using sklearn to convert them to one hot encoding representation.  \n",
    "\n",
    "#### We launch 4 PARALLEL jobs and hence we seemingly create 4 Transformers, but in reality they are just the SAME estimator being run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/sklearn_preprocessor_batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/sklearn_preprocessor_batch.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    content_types,\n",
    "    encoders,\n",
    "    env,\n",
    "    modules,\n",
    "    transformer,\n",
    "    worker,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Binarizer, OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Since we get a headerless CSV file we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"YEAR_BUILT\",\n",
    "    \"SQUARE_FEET\",\n",
    "    \"NUM_BEDROOMS\",\n",
    "    \"NUM_BATHROOMS\",\n",
    "    \"LOT_ACRES\",\n",
    "    \"GARAGE_SPACES\",\n",
    "    \"FRONT_PORCH\",\n",
    "    \"DECK\",\n",
    "]\n",
    "\n",
    "label_column = \"PRICE\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"YEAR_BUILT\": str,\n",
    "    \"SQUARE_FEET\": np.float64,\n",
    "    \"NUM_BEDROOMS\": np.float64,\n",
    "    \"NUM_BATHROOMS\": np.float64,\n",
    "    \"LOT_ACRES\": np.float64,\n",
    "    \"GARAGE_SPACES\": np.float64,\n",
    "    \"FRONT_PORCH\": str,\n",
    "    \"DECK\": str,\n",
    "}\n",
    "\n",
    "label_column_dtype = {\"PRICE\": np.float64}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [os.path.join(args.train, file) for file in os.listdir(args.train)]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"There are no files in {}.\\n\"\n",
    "                + \"This usually indicates that the train channel was incorrectly specified,\\n\"\n",
    "                + \"the data specification in S3 was incorrectly specified or the role specified\\n\"\n",
    "                + \"does not have permission to access the data.\".format(args.train)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for file in input_files:\n",
    "        print(\"file :\", file)\n",
    "\n",
    "    raw_data = [pd.read_csv(file, header=None, names=feature_columns_names + [label_column])]\n",
    "\n",
    "    concat_data = pd.concat(raw_data)\n",
    "\n",
    "    print(concat_data)\n",
    "\n",
    "    # This section is adapted from the scikit-learn example of using preprocessing pipelines:\n",
    "    #\n",
    "    # https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "    #\n",
    "\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"FRONT_PORCH\")\n",
    "    numeric_features.remove(\"DECK\")\n",
    "    numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "    categorical_features = [\"FRONT_PORCH\", \"DECK\"]\n",
    "    categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    preprocessor.fit(concat_data)\n",
    "    \n",
    "    joblib.dump(preprocessor, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "    print(\"saved model!\")\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == \"text/csv\":\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), header=None)\n",
    "\n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example, includes the ring label\n",
    "            df.columns = feature_columns_names + [label_column]\n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({\"features\": row})\n",
    "\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == \"text/csv\":\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "\n",
    "    We implement this because the default uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "\n",
    "    The output is returned in the following order:\n",
    "\n",
    "        rest of features either one hot encoded or standardized\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Input data type \", type(input_data))\n",
    "\n",
    "    print(input_data)\n",
    "\n",
    "    features = model.transform(input_data)\n",
    "\n",
    "    print(\"features type \", type(features))\n",
    "\n",
    "    print(features)\n",
    "\n",
    "    features_array = features\n",
    "\n",
    "    print(\"features_array \", type(features_array))\n",
    "\n",
    "    print(features_array)\n",
    "\n",
    "    if label_column in input_data:\n",
    "        # Return the label (as the first column) and the set of features.\n",
    "        return np.insert(features_array, 0, input_data[label_column], axis=1)\n",
    "    else:\n",
    "        # Return only the set of features\n",
    "        return features\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\"\"\"\n",
    "    preprocessor = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the SKLearn estimator with the sklearn_preprocessor.py as the script\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'scripts/sklearn_preprocessor_batch.py'\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    framework_version=\"0.20.0\",\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-333262794411/housing-data/NewYork_NY/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-333262794411/housing-data/LosAngeles_CA/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-333262794411/housing-data/Chicago_IL/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-333262794411/housing-data/Houston_TX/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "#Upload the raw training data to S3 bucket, to be accessed by SKLearn\n",
    "train_inputs = []\n",
    "PARALLEL_TRAINING_JOBS=4\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "\n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path='data/{}/train/train.csv'.format(loc),\n",
    "        bucket=BUCKET,\n",
    "        key_prefix='housing-data/{}/train'.format(loc)\n",
    "    )\n",
    "    \n",
    "    train_inputs.append(train_input)\n",
    "    print(\"Raw training data uploaded to : \", train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn_estimator fit input data at  0  for loc  NewYork_NY\n",
      "sklearn_estimator fit input data at  1  for loc  LosAngeles_CA\n",
      "sklearn_estimator fit input data at  2  for loc  Chicago_IL\n",
      "sklearn_estimator fit input data at  3  for loc  Houston_TX\n"
     ]
    }
   ],
   "source": [
    "##Launch multiple scikit learn training to process the raw synthetic data generated for multiple locations.\n",
    "##Before executing this, take the training instance limits in your account and cost into consideration.\n",
    "\n",
    "sklearn_estimators = []\n",
    "sklearn_estimator_jobs = []\n",
    "\n",
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    "    print(\"sklearn_estimator fit input data at \", index , \" for loc \", loc)\n",
    "     \n",
    "    job_name='scikit-learnestimator-{}'.format(strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "    \n",
    "    sklearn_estimator.fit({'train': train_inputs[index]}, job_name=job_name, wait=False)\n",
    "\n",
    "    sklearn_estimators.append(sklearn_estimator)\n",
    "    sklearn_estimator_jobs.append(job_name)\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wait for the preprocessor jobs to finish\n",
    "for job_name in sklearn_estimator_jobs:\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    \n",
    "    waiter = sm_client.get_waiter('training_job_completed_or_stopped')\n",
    "    waiter.wait(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Bring your own Model\n",
    "Here we will work on the tar file created from the training job and create all \n",
    "needed jobs definetions from scratch and run the transform job. We will run 4 different kinds\n",
    "\n",
    "    Run with the Input filter not There so only generate predictions\n",
    "    Run with Input filter values so we can generate predictions and also combine to the outputs\n",
    "    Run with Mini batch and instance count > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a ) We show how to create 'n' BATCH Job Transformer from the Estimator Object\n",
    "All of these will run in Parallel there by saving time but leverage the same model which has been trained\n",
    "Here we already have the Estimator with the Inference file with the model definetions\n",
    "we will leverage that to create a transformer and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLEL_BATCH_JOBS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform the raw data at  0  for loc  NewYork_NY\n"
     ]
    }
   ],
   "source": [
    "##Once the preprocessor is fit, use tranformer to preprocess the raw training data and store the transformed data right back into s3.\n",
    "##Before executing this, take the training instance limits in your account and cost into consideration.\n",
    "\n",
    "sklearn_estimator_transformers = []\n",
    "\n",
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_BATCH_JOBS]):\n",
    "    print(\"Transform the raw data at \", index , \" for loc \", loc)\n",
    "       \n",
    "    sklearn_estimator = sklearn_estimators[index]\n",
    "    \n",
    "    transformer = sklearn_estimator.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        assemble_with='Line',\n",
    "        accept='text/csv'\n",
    "    )\n",
    "    \n",
    "    sklearn_estimator_transformers.append(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING: batch transform job: sagemaker-scikit-learn-2022-08-13-22-22-20-678\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training input\n",
    "preprocessed_train_data_path = []\n",
    "\n",
    "for index, transformer in enumerate(sklearn_estimator_transformers):\n",
    "    transformer.transform(train_inputs[index], content_type='text/csv', wait=False)\n",
    "    print('STARTING: batch transform job: {}'.format(transformer.latest_transform_job.job_name))\n",
    "    preprocessed_train_data_path.append(transformer.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRANSFORM job sagemaker-scikit-learn-2022-08-13-22-22-20-678 to complete...\n"
     ]
    }
   ],
   "source": [
    "#Wait for all the batch transform jobs to finish\n",
    "for transformer in sklearn_estimator_transformers: \n",
    "    job_name=transformer.latest_transform_job.job_name\n",
    "    print('Waiting for TRANSFORM job {} to complete...'.format(job_name))\n",
    "    \n",
    "    waiter = sm_client.get_waiter('transform_job_completed_or_stopped')\n",
    "    waiter.wait(TransformJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1b) Bring your own Model as tar ball in S3\n",
    "Here we will print the newest location of the Model in case needed to be used for Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using location of the TAR ball from s3://sagemaker-us-east-1-333262794411/scikit-learnestimator-2022-08-13-22-09-17/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using location of the TAR ball from {sklearn_estimators[0].model_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_new_loc = sklearn_estimators[0].model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2b ) Bring your own Model as tar ball in S3\n",
    "Here we will use the Tar ball as is and then create all the required artifacts from scratch\n",
    "    First we upload the Model tar ball to S3 to be used in our Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the RAW data set to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-333262794411/housing-data/NewYork_NY/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "#Upload the raw training data to S3 bucket, to be accessed by SKLearn\n",
    "train_inputs = []\n",
    "PARALLEL_TRAINING_JOBS=1\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "\n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path='data/{}/train/train.csv'.format(loc),\n",
    "        bucket=BUCKET,\n",
    "        key_prefix='housing-data/{}/train'.format(loc)\n",
    "    )\n",
    "    \n",
    "    train_inputs.append(train_input)\n",
    "    print(\"Raw training data uploaded to : \", train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the Model as tar ball into S3 location for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-333262794411/byom/scikit-learnestimator/model/batch\n",
      "s3://sagemaker-us-east-1-333262794411/byom/scikit-learnestimator/model/batch/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# - UPLOAD the MODEL to S3\n",
    "desired_model_s3 = 's3://{}/{}'.format(sagemaker_session.default_bucket(),'byom/scikit-learnestimator/model/batch')\n",
    "print(desired_model_s3)\n",
    "model_s3_upload=sagemaker.s3.S3Uploader().upload(local_path='./models/batch/model.tar.gz', desired_s3_uri=desired_model_s3,sagemaker_session=sagemaker_session)  \n",
    "print(model_s3_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the BATCH Jobs:\n",
    "\n",
    "The steps to create the Job is straight forward\n",
    "* Create a Model object from the S3 location with the Image\n",
    "* Create a Transformer \n",
    "* Run the Transform with the S3 INPUT location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Container image\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework=\"sklearn\", version=\"0.20.0\") # 0.23-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the model from s3=s3://sagemaker-us-east-1-333262794411/byom/scikit-learnestimator/model/batch/model.tar.gz:\n"
     ]
    }
   ],
   "source": [
    "print(f\"using the model from s3={model_s3_upload}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "scripts/sklearn_preprocessor_batch.py\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "sklearn_model_name = \"DEMO-BATCH-SKLEARN-BYO-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "sklearn_model = SKLearnModel(\n",
    "    name=sklearn_model_name,\n",
    "    model_data=model_s3_upload, #model_data_new_loc, \n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"scripts/sklearn_preprocessor_batch.py\",\n",
    "    framework_version=\"0.20.0\", #\"0.23-1\", #\"0.20.0\",\n",
    "    image_uri=container,\n",
    "    #source_dir=\"scripts\",\n",
    ")\n",
    "print(sklearn_model.source_dir)\n",
    "print(sklearn_model.entry_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transformer = sklearn_model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        assemble_with='Line',\n",
    "        accept='text/csv'\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................\u001b[34mProcessing /opt/ml/code\n",
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor-batch\n",
      "  Building wheel for sklearn-preprocessor-batch (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor-batch (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor-batch: filename=sklearn_preprocessor_batch-1.0.0-py2.py3-none-any.whl size=7596 sha256=867f39f97cff021cd88133fbe6809e91c0a46a97a24756b090aaf82ce2dd34c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pmw624m9/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor-batch\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor-batch\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-batch-1.0.0\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sklearn-preprocessor-batch\n",
      "  Building wheel for sklearn-preprocessor-batch (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor-batch (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor-batch: filename=sklearn_preprocessor_batch-1.0.0-py2.py3-none-any.whl size=7596 sha256=867f39f97cff021cd88133fbe6809e91c0a46a97a24756b090aaf82ce2dd34c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pmw624m9/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built sklearn-preprocessor-batch\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sklearn-preprocessor-batch\u001b[0m\n",
      "\u001b[35mSuccessfully installed sklearn-preprocessor-batch-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [37] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37)\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [37] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [45] [INFO] Booting worker with pid: 45\u001b[0m\n",
      "\u001b[34m[2022-08-13 22:18:32 +0000] [46] [INFO] Booting worker with pid: 46\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [37] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37)\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [37] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [45] [INFO] Booting worker with pid: 45\u001b[0m\n",
      "\u001b[35m[2022-08-13 22:18:32 +0000] [46] [INFO] Booting worker with pid: 46\u001b[0m\n",
      "\u001b[34m2022-08-13 22:18:36,304 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2022-08-13 22:18:36,304 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [13/Aug/2022:22:18:36 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [13/Aug/2022:22:18:36 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2022-08-13 22:18:36,982 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [13/Aug/2022:22:18:36 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [13/Aug/2022:22:18:36 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2022-08-13 22:18:36,982 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          2003  2267.528685             3  ...            n     n  350579\u001b[0m\n",
      "\u001b[34m1          2003  4971.666330             5  ...            n     y  807999\u001b[0m\n",
      "\u001b[34m2          2007  3675.720185             6  ...            y     y  646958\u001b[0m\n",
      "\u001b[34m3          1986  3318.806832             3  ...            y     y  422121\u001b[0m\n",
      "\u001b[34m4          1998  3977.783049             4  ...            y     y  613917\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1990  2471.179794             3  ...            n     y  308926\u001b[0m\n",
      "\u001b[34m896        1997  4206.121206             5  ...            y     y  668768\u001b[0m\n",
      "\u001b[34m897        2001  2792.078359             4  ...            n     n  445461\u001b[0m\n",
      "\u001b[34m898        2005  3295.155759             4  ...            y     y  526573\u001b[0m\n",
      "\u001b[35mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[35m0          2003  2267.528685             3  ...            n     n  350579\u001b[0m\n",
      "\u001b[35m1          2003  4971.666330             5  ...            n     y  807999\u001b[0m\n",
      "\u001b[35m2          2007  3675.720185             6  ...            y     y  646958\u001b[0m\n",
      "\u001b[35m3          1986  3318.806832             3  ...            y     y  422121\u001b[0m\n",
      "\u001b[35m4          1998  3977.783049             4  ...            y     y  613917\u001b[0m\n",
      "\u001b[35m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[35m895        1990  2471.179794             3  ...            n     y  308926\u001b[0m\n",
      "\u001b[35m896        1997  4206.121206             5  ...            y     y  668768\u001b[0m\n",
      "\u001b[35m897        2001  2792.078359             4  ...            n     n  445461\u001b[0m\n",
      "\u001b[35m898        2005  3295.155759             4  ...            y     y  526573\u001b[0m\n",
      "\u001b[34m899        1988  3103.512899             5  ...            n     y  437776\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[34mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 0.8228916  -0.96642891 -0.71577179 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.8228916   2.60139069  0.69693569 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.21484843  0.89152874  1.40328943 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.62691318 -0.27434188 -0.00941805 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.01887001  0.3894148  -0.00941805 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.64694653  0.1365626   0.69693569 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[ 0.8228916  -0.96642891 -0.71577179 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.8228916   2.60139069  0.69693569 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.21484843  0.89152874  1.40328943 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.62691318 -0.27434188 -0.00941805 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.01887001  0.3894148  -0.00941805 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.64694653  0.1365626   0.69693569 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [13/Aug/2022:22:18:37 +0000] \"POST /invocations HTTP/1.1\" 200 128810 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m899        1988  3103.512899             5  ...            n     y  437776\u001b[0m\n",
      "\u001b[35m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[35mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 0.8228916  -0.96642891 -0.71577179 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.8228916   2.60139069  0.69693569 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.21484843  0.89152874  1.40328943 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.62691318 -0.27434188 -0.00941805 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.01887001  0.3894148  -0.00941805 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.64694653  0.1365626   0.69693569 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[35mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[35m[[ 0.8228916  -0.96642891 -0.71577179 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.8228916   2.60139069  0.69693569 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 1.21484843  0.89152874  1.40328943 ...  1.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.62691318 -0.27434188 -0.00941805 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 1.01887001  0.3894148  -0.00941805 ...  1.          0.\n",
      "   1.        ]\n",
      " [-0.64694653  0.1365626   0.69693569 ...  0.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [13/Aug/2022:22:18:37 +0000] \"POST /invocations HTTP/1.1\" 200 128810 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2022-08-13T22:18:36.942:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_transformer.transform(\n",
    "    train_inputs[0], \n",
    "    content_type='text/csv', \n",
    "    wait=True, \n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we list the data and view it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-333262794411/DEMO-BATCH-SKLEARN-BYO-2022-08-13-22-12-2022-08-13-22-12-55-357/train.csv.out'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_file_name = '{}/train.csv.out'.format(batch_transformer.output_path)\n",
    "out_file_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350579.0</td>\n",
       "      <td>0.822892</td>\n",
       "      <td>-0.966429</td>\n",
       "      <td>-0.715772</td>\n",
       "      <td>1.367619</td>\n",
       "      <td>0.144956</td>\n",
       "      <td>-1.312254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>807999.0</td>\n",
       "      <td>0.822892</td>\n",
       "      <td>2.601391</td>\n",
       "      <td>0.696936</td>\n",
       "      <td>0.670644</td>\n",
       "      <td>2.512096</td>\n",
       "      <td>0.451792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>646958.0</td>\n",
       "      <td>1.214848</td>\n",
       "      <td>0.891529</td>\n",
       "      <td>1.403289</td>\n",
       "      <td>-0.026330</td>\n",
       "      <td>0.183135</td>\n",
       "      <td>0.451792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>422121.0</td>\n",
       "      <td>-0.842925</td>\n",
       "      <td>0.420620</td>\n",
       "      <td>-0.715772</td>\n",
       "      <td>-0.723305</td>\n",
       "      <td>0.488573</td>\n",
       "      <td>-1.312254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>613917.0</td>\n",
       "      <td>0.332946</td>\n",
       "      <td>1.290068</td>\n",
       "      <td>-0.009418</td>\n",
       "      <td>1.367619</td>\n",
       "      <td>0.603112</td>\n",
       "      <td>-1.312254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    7   \\\n",
       "0  350579.0  0.822892 -0.966429 -0.715772  1.367619  0.144956 -1.312254  1.0   \n",
       "1  807999.0  0.822892  2.601391  0.696936  0.670644  2.512096  0.451792  1.0   \n",
       "2  646958.0  1.214848  0.891529  1.403289 -0.026330  0.183135  0.451792  0.0   \n",
       "3  422121.0 -0.842925  0.420620 -0.715772 -0.723305  0.488573 -1.312254  0.0   \n",
       "4  613917.0  0.332946  1.290068 -0.009418  1.367619  0.603112 -1.312254  0.0   \n",
       "\n",
       "    8    9    10  \n",
       "0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  1.0  \n",
       "2  1.0  0.0  1.0  \n",
       "3  1.0  0.0  1.0  \n",
       "4  1.0  0.0  1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - download the file\n",
    "sagemaker.s3.S3Downloader().download(s3_uri=out_file_name, local_path='./data/output', sagemaker_session=sagemaker_session)  \n",
    "output_df = pd.read_csv(filepath_or_buffer='./data/output/train.csv.out', header=None)\n",
    "output_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Transformer with a JOIN to the INPUT DATA set with column 'Year_built'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: DEMO-BATCH-SKLEARN-BYO-2022-08-05-01-18-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................\u001b[34mProcessing /opt/ml/code\n",
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor-batch\n",
      "  Building wheel for sklearn-preprocessor-batch (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor-batch (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor-batch: filename=sklearn_preprocessor_batch-1.0.0-py2.py3-none-any.whl size=7596 sha256=939b914cf2c0c3b9a72d12f32ec6aba7fa608835c278397e7df8b134fa47f47b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_1i4_t7o/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor-batch\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor-batch\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-batch-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [37] [INFO] Starting gunicorn 20.1.0\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37)\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [37] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2022-08-05 02:25:09 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m2022-08-05 02:25:13,489 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Aug/2022:02:25:14 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2022-08-05 02:25:14,153 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Aug/2022:02:25:14 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2022-08-05 02:25:14,864 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mInput data type  <class 'pandas.core.frame.DataFrame'>\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          1987  2369.578072             4  ...            n     n  265736\u001b[0m\n",
      "\u001b[34m1          1983  3143.137014             2  ...            y     n  391170\u001b[0m\n",
      "\u001b[34m2          1990  3030.147148             6  ...            n     y  438672\u001b[0m\n",
      "\u001b[34m3          2003  4062.525686             6  ...            n     n  646078\u001b[0m\n",
      "\u001b[34m4          1979  2683.494602             6  ...            n     y  349224\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        2018  2264.658702             4  ...            y     y  484998\u001b[0m\n",
      "\u001b[34m896        1989  2377.565309             2  ...            n     y  299384\u001b[0m\n",
      "\u001b[34m897        1985  3118.239782             6  ...            n     y  394035\u001b[0m\n",
      "\u001b[34m898        1995  3713.245069             6  ...            n     n  556386\u001b[0m\n",
      "\u001b[34m899        2001  3690.564328             6  ...            y     y  617834\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\u001b[0m\n",
      "\u001b[34mfeatures type  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[-0.74493574 -0.83178569 -0.00941805 ...  0.          1.\n",
      "   0.        ]\n",
      " [-1.13689257  0.18884237 -1.42212553 ...  1.          1.\n",
      "   0.        ]\n",
      " [-0.45096811  0.03976436  1.40328943 ...  0.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.94091416  0.15599314  1.40328943 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.03897793  0.9410388   1.40328943 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.62691318  0.911114    1.40328943 ...  1.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34mfeatures_array  <class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34m[[-0.74493574 -0.83178569 -0.00941805 ...  0.          1.\n",
      "   0.        ]\n",
      " [-1.13689257  0.18884237 -1.42212553 ...  1.          1.\n",
      "   0.        ]\n",
      " [-0.45096811  0.03976436  1.40328943 ...  0.          0.\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.94091416  0.15599314  1.40328943 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.03897793  0.9410388   1.40328943 ...  0.          1.\n",
      "   0.        ]\n",
      " [ 0.62691318  0.911114    1.40328943 ...  1.          0.\n",
      "   1.        ]]\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [05/Aug/2022:02:25:15 +0000] \"POST /invocations HTTP/1.1\" 200 128841 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[32m2022-08-05T02:25:14.760:[sagemaker logs]: MaxConcurrentTransforms=8, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "batch_transformer = sklearn_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    assemble_with='Line',\n",
    "    accept='text/csv',\n",
    "    max_concurrent_transforms=8,\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=6,\n",
    ")\n",
    "\n",
    "batch_transformer.transform(\n",
    "    train_inputs[index], \n",
    "    content_type='text/csv', \n",
    "    input_filter=None,\n",
    "    join_source=\"Input\",\n",
    "    output_filter='$[0,-11]',\n",
    "    split_type='Line',\n",
    "    wait=True, \n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/DEMO-BATCH-SKLEARN-BYO-2022-08-05-01-18-2022-08-05-02-19-30-100/train.csv.out\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>265736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1983</td>\n",
       "      <td>391170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>438672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>646078.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979</td>\n",
       "      <td>349224.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1\n",
       "0  1987  265736.0\n",
       "1  1983  391170.0\n",
       "2  1990  438672.0\n",
       "3  2003  646078.0\n",
       "4  1979  349224.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_file_name = '{}/train.csv.out'.format(batch_transformer.output_path)\n",
    "print(out_file_name)\n",
    "# - download the file\n",
    "sagemaker.s3.S3Downloader().download(s3_uri=out_file_name, local_path='./data/output', sagemaker_session=sagemaker_session)  \n",
    "output_df2 = pd.read_csv(filepath_or_buffer='./data/output/train.csv.out', header=None)\n",
    "output_df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up<a id='CleanUp'></a>\n",
    "Clean up the endpoint to avoid unneccessary costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the endpoint and underlying model\n",
    "predictor.delete_model() \n",
    "predictor.delete_endpoint()\n",
    "for t in preprocessor_transformers:\n",
    "    t.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
