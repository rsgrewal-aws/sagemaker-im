{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62425a7",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DeepSpeed Container. In this notebook we show Bloom-176B model hosting\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using DeepSpeed and DJL. DJL provides for the serving framework while DeepSpeed is the key sharding library we leverage to enable hosting of large models. We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "Language models have recently exploded in both size and popularity. In 2018, BERT-large entered the scene and, with its 340M parameters and novel transformer architecture, set the standard on NLP task accuracy. Within just a few years, state-of-the-art NLP model size has grown by more than 500x with models such as OpenAI’s 175 billion parameter GPT-3 and similarly sized open source Bloom 176B raising the bar on NLP accuracy. This increase in the number of parameters is driven by the simple and empirically-demonstrated positive relationship between model size and accuracy: more is better. With easy access from models zoos such as Hugging Face and improved accuracy in NLP tasks such as classification and text generation, practitioners are increasingly reaching for these large models. However, deploying them can be a challenge because of their size.\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy the open source Bloom 176B quantized model across GPU's on a ml.p4d.24xlarge instance. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to https://arxiv.org/pdf/2207.00032.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd36c97",
   "metadata": {},
   "source": [
    "## Licence agreement\n",
    "View license information https://huggingface.co/spaces/bigscience/license for this model including the use-based restrictions in Section 5 before using the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84baef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instal boto3 library to create model and run inference workloads\n",
    "%pip install -Uqq boto3 awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968350fe",
   "metadata": {},
   "source": [
    "## Optional Section to Download Model from Hugging Face Hub\n",
    "\n",
    "Use this section of you are interested in downloading the model directly from Huggingface hub and storing in your own S3 bucket. Please change the variable \"install_model_locally\" to True in that case.\n",
    "\n",
    "**However this notebook currently leverages the model stored in AWS public S3 location for ease of use. So you can skip this step**\n",
    "\n",
    "The below step to download and then upload to S3 can take several minutes since the model size is extremely large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "install_model_locally=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install_model_locally:\n",
    "    %pip install huggingface-hub -Uqq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466569a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install_model_locally:\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "    from pathlib import Path\n",
    "    \n",
    "    import sagemaker\n",
    "    bucket = sagemaker.session.Session().default_bucket()\n",
    "    \n",
    "    # - This will download the model into the ./model directory where ever the jupyter file is running\n",
    "    local_model_path = Path(\"./model\")\n",
    "    local_model_path.mkdir(exist_ok=True)\n",
    "    model_name = \"microsoft/bloom-deepspeed-inference-int8\"\n",
    "    commit_hash = \"aa00a6626f6484a2eef68e06d1e089e4e32aa571\"\n",
    "    \n",
    "    # - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "    snapshot_download(repo_id=model_name, revision=commit_hash, cache_dir=local_model_path)\n",
    "    \n",
    "    # - Upload to S3 using AWS CLI \n",
    "    s3_model_prefix = \"hf-large-model-djl-ds/model\"  # folder where model checkpoint will go\n",
    "    model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "    \n",
    "    !aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb0322",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. In the Inference code we kick of a multi threaded approach to download the model weights into the container using awscli\n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "The actual model is stored in S3 location and will be downloaded into the container directly when the endpoint is created. For that we will pass in two environment variables\n",
    "\n",
    "1.  \"MODEL_S3_BUCKET\" Specify the S3 Bucket where the model artifact is\n",
    "2.  \"MODEL_S3_PREFIX\" Specify the S3 prefix for where the model artifacts file are actually located\n",
    "\n",
    "This will be used in the model.py file to read in the actual model artifacts. \n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. It is also responsible for loading the model from S3\n",
    "- `requirements.txt` has the awscli library needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c61ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e4213",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e9844326",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = \"sagemaker-sample-files\"\n",
    "s3_code_prefix = \"hf-large-model-djl-ds/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"models/bloom-176B/raw_model_microsoft/\" # \"bloom-176B/raw_model_microsoft/\"  # folder where model checkpoint will go\n",
    "# --  s3://sagemaker-sample-files/models/bloom-176B/raw_model_microsoft/ -\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ccc45a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437f7d1",
   "metadata": {},
   "source": [
    "**Image URI of the DJL Container to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b5858cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.19.0-deepspeed0.7.3-cu113\n"
     ]
    }
   ],
   "source": [
    "#inference_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/djl-ds:latest\"\n",
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.19.0-deepspeed0.7.3-cu113\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70197327",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "961ee907",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code_bloom176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e74c5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code_bloom176/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code_bloom176/model.py\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "def check_config():\n",
    "    local_rank = os.getenv('LOCAL_RANK')\n",
    "    curr_pid = os.getpid()\n",
    "    print(f'__Number CUDA Devices:{torch.cuda.device_count()}:::local_rank={local_rank}::curr_pid={curr_pid}::')\n",
    "    \n",
    "    if not local_rank:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    if not check_config():\n",
    "        raise Exception(\"DJL:DeepSpeed configurations are not default. This code does not support non default configurations\") \n",
    "        \n",
    "    deepspeed.init_distributed(\"nccl\")\n",
    "    \n",
    "    tensor_parallel = int(os.getenv('TENSOR_PARALLEL_DEGREE', '1'))\n",
    "    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "    model_dir = \"/tmp/model\"\n",
    "    bucket = os.environ.get(\"MODEL_S3_BUCKET\")\n",
    "    key_prefix = os.environ.get(\"MODEL_S3_PREFIX\")\n",
    "    curr_pid = os.getpid()\n",
    "    print(f'tensor_parallel={tensor_parallel}::curr_pid={curr_pid}::')\n",
    "    print(f\"Current Rank: {local_rank}:: pid={curr_pid}::Going to load the model weights on rank 0: bucket={bucket}::key={key_prefix}::\")\n",
    "    \n",
    "    if local_rank == 0: \n",
    "            \n",
    "        if f\"{model_dir}/DONE\" not in glob(f\"{model_dir}/*\"):\n",
    "            print(f\"Starting Model downloading files pid={curr_pid}::\")\n",
    "            print(f\"Starting Model pid={curr_pid}::\")\n",
    "            \n",
    "            try:\n",
    "                # -- \n",
    "                proc_run = subprocess.run([\"aws\", \"s3\", \"cp\", \"--recursive\", f\"s3://{bucket}/{key_prefix}\", model_dir], capture_output=True, text=True) # python 7 onwards\n",
    "                print(f\"Model download finished: pid={curr_pid}::\")\n",
    "                \n",
    "                # write file when download complete. Could use dist.barrier() but this makes it easier to check if model is downloaded in case of retry \n",
    "                with open(f\"{model_dir}/DONE\", \"w\") as f:\n",
    "                    f.write(\"download_complete\")\n",
    "\n",
    "                print(f\"Model download checkmark written out pid={curr_pid}::return_code:{proc_run.returncode}:stderr:-- >:{proc_run.stderr}\")\n",
    "                proc_run.check_returncode() # to throw the error in case there was one\n",
    "                \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print ( \"Model download failed: Error:\\nreturn code: \", e.returncode, \"\\nOutput: \", e.stderr )\n",
    "                raise # FAIL FAST \n",
    "                \n",
    "    dist.barrier() # - to ensure all processes load fine\n",
    "        \n",
    "    print(f\"Load the Model  pid={curr_pid}::\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    # has to be FP16 as Int8 model loading not yet supported\n",
    "    with deepspeed.OnDevice(dtype=torch.float16, device=\"meta\"):\n",
    "        model = AutoModelForCausalLM.from_config(\n",
    "            AutoConfig.from_pretrained(model_dir), torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "    model = model.eval()\n",
    "    \n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=torch.int8,\n",
    "        base_dir = model_dir,\n",
    "        checkpoint=os.path.join(model_dir, \"ds_inference_config.json\"),\n",
    "        replace_method='auto',\n",
    "        replace_with_kernel_inject=True\n",
    "    )\n",
    "\n",
    "    model = model.module\n",
    "    dist.barrier()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    print(\"Model In handle\")\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = get_model()\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        print(\"Model warm up: inputs were empty:called by Model server to warmup\")\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    \n",
    "    inputs = inputs.get_as_json()\n",
    "    \n",
    "    #print(inputs)\n",
    "    data = inputs[\"input\"]\n",
    "    generate_kwargs = inputs.get(\"gen_kwargs\", {})\n",
    "    padding = bool(inputs.get(\"padding\", 'True') )\n",
    "    \n",
    "    start = time.time() \n",
    "    input_tokens = tokenizer(data, return_tensors=\"pt\", padding=padding)\n",
    "    print(len(input_tokens))\n",
    "    \n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to(torch.cuda.current_device())\n",
    "    #print(f\"Model:Tokenizer:ENCODE:time:{(time.time() - start) * 1000} ms\")\n",
    "    \n",
    "    start = time.time()    \n",
    "    with torch.no_grad():\n",
    "        generate_kwargs.pop('padding', None)\n",
    "        output = model.generate(**input_tokens, **generate_kwargs)\n",
    "    #print(output)\n",
    "    print(f\"Model:Prediction:time:{(time.time() - start) * 1000} ms\")\n",
    "    \n",
    "    start = time.time()\n",
    "    output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    #print(f\"Model:Tokenizer:DECODE:time:{(time.time() - start) * 1000} ms\")\n",
    "    \n",
    "    torch.cuda.empy_cache() # to fre up the memory\n",
    "    return Output().add_as_json(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "776698fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True <class 'bool'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_length': 5,\n",
       " 'max_new_tokens': 100,\n",
       " 'temperature': 0.8,\n",
       " 'num_beams': 5,\n",
       " 'no_repeat_ngram_size': 2}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "                \"min_length\": 5,\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.8,\n",
    "                \"num_beams\": 5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"padding\":'True',\n",
    "                \"padding\" : True,\n",
    "}\n",
    "\n",
    "gen_kwargs.pop('padding', None)\n",
    "    \n",
    "print(bool('True'), bool('true'), type(bool('True')))\n",
    "gen_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27134a0b",
   "metadata": {},
   "source": [
    "#### with Pipeline Batch size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04ce6d",
   "metadata": {},
   "source": [
    "#### Serving.properties has engine parameter which tells the DJL model server to use the DeepSpeed engine to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d38e73e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code_bloom176/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile code_bloom176/serving.properties\n",
    "engine=DeepSpeed\n",
    "batch_size=50\n",
    "batchSize=50\n",
    "BATCH_SIZE=50\n",
    "max_batch_delay=10\n",
    "maxBatchDelay=10\n",
    "MAX_BATCH_DELAY=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b656b",
   "metadata": {},
   "source": [
    "#### Requirements will tell the container to load these additional libraries into the container. We need these to download the model into the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0354cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code_bloom176/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code_bloom176/requirements.txt\n",
    "boto3\n",
    "awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "63031483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_bloom176/\n",
      "code_bloom176/serving.properties\n",
      "code_bloom176/model.py\n",
      "code_bloom176/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz code_bloom176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a58d7e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-622343165275/hf-large-model-djl-ds/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f6f0c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Model Prefix where the model files are -- > models/bloom-176B/raw_model_microsoft/\n",
      "S3 Model Bucket is -- > sagemaker-sample-files\n"
     ]
    }
   ],
   "source": [
    "print(f\"S3 Model Prefix where the model files are -- > {s3_model_prefix}\")\n",
    "print(f\"S3 Model Bucket is -- > {model_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea7982",
   "metadata": {},
   "source": [
    "### This is optional in case you want to use VpcConfig to specify when creating the end points\n",
    "\n",
    "For more details you can refer to this link https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html\n",
    "\n",
    "The below is just an example to extract information about Security Groups and Subnets needed to configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3ab4d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `|'\n",
      "/bin/bash: -c: line 0: `aws ec2 describe-security-groups --filter Name=vpc-id,Values=<use vpcId> | python3 -c \"import sys, json; print(json.load(sys.stdin)['SecurityGroups'])\"'\n"
     ]
    }
   ],
   "source": [
    "!aws ec2 describe-security-groups --filter Name=vpc-id,Values=<use vpcId> | python3 -c \"import sys, json; print(json.load(sys.stdin)['SecurityGroups'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - provide networking configs if needed.\n",
    "security_group_ids = []  # add the security group id's\n",
    "subnets = []  # add the subnet id for this vpc\n",
    "privateVpcConfig = {\"SecurityGroupIds\": security_group_ids, \"Subnets\": subnets}\n",
    "print(privateVpcConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b9d35",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.p4d.24xlarge \n",
    "    \n",
    "    b) ModelDataDownloadTimeoutInSeconds is 2400 which is needed to ensure the Model downloads from S3 successfully,\n",
    "    \n",
    "    c) ContainerStartupHealthCheckTimeoutInSeconds is 2400 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80d5ec",
   "metadata": {},
   "source": [
    "One of the key parameters here is **TENSOR_PARALLEL_DEGREE** which essentially tells the DeepSpeed library to partition the models along 8 GPU's. This is a tunable and configurable parameter. For the purpose of this notebook we would like to leave these as **default settings**.\n",
    "\n",
    "This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeedyou can follow the link https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "70b57115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom-djl-ds-2022-12-05-17-11-54-151\n",
      "Created Model: arn:aws:sagemaker:us-east-1:622343165275:model/bloom-djl-ds-2022-12-05-17-11-54-151\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"bloom-djl-ds\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "        \"Environment\": {\n",
    "            \"MODEL_S3_BUCKET\": model_bucket,\n",
    "            \"MODEL_S3_PREFIX\": s3_model_prefix,\n",
    "            \"TENSOR_PARALLEL_DEGREE\": \"8\",\n",
    "        },\n",
    "    },\n",
    "    # Uncomment if providing networking configs\n",
    "    # VpcConfig=privateVpcConfig\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742cc63d",
   "metadata": {},
   "source": [
    "VolumnSizeInGB has been left as commented out. You should use this value for Instance types which support EBS volume mounts. The current instance we are using comes with a pre configured space and does not support additional volume mounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "51e89bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint-config/bloom-djl-ds-2022-12-05-17-11-54-151-config',\n",
       " 'ResponseMetadata': {'RequestId': '8e35a899-486c-4c6a-96d1-fe97b3b881bc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8e35a899-486c-4c6a-96d1-fe97b3b881bc',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '124',\n",
       "   'date': 'Mon, 05 Dec 2022 17:11:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.p4d.24xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"VolumeSizeInGB\" : 400,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ae736f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-djl-ds-2022-12-05-17-11-54-151-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf7d9f",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created.This can be take couple of minutes or longer. Please be patient\n",
    "However while that happens, let us look at the critical areas of the helper files we are using to load the model\n",
    "1. We will look at the code snippets for model.py to see the model downloading mechanism\n",
    "2. Requirements.txt to see the required libraries to be loaded\n",
    "3. Serving.properties to see the environment related properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "137f8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    print(f\"Current Rank: {local_rank}:: pid={curr_pid}::Going to load the model weights on rank 0: bucket={bucket}::key={key_prefix}::\")\n",
      "    \n",
      "    if local_rank == 0: \n",
      "            \n",
      "        if f\"{model_dir}/DONE\" not in glob(f\"{model_dir}/*\"):\n",
      "            print(f\"Starting Model downloading files pid={curr_pid}::\")\n",
      "            print(f\"Starting Model pid={curr_pid}::\")\n",
      "            \n",
      "            try:\n",
      "                # -- \n",
      "                proc_run = subprocess.run([\"aws\", \"s3\", \"cp\", \"--recursive\", f\"s3://{bucket}/{key_prefix}\", model_dir], capture_output=True, text=True) # python 7 onwards\n",
      "                print(f\"Model download finished: pid={curr_pid}::\")\n",
      "                \n",
      "                # write file when download complete. Could use dist.barrier() but this makes it easier to check if model is downloaded in case of retry \n",
      "                with open(f\"{model_dir}/DONE\", \"w\") as f:\n",
      "                    f.write(\"download_complete\")\n",
      "\n",
      "                print(f\"Model download checkmark written out pid={curr_pid}::return_code:{proc_run.returncode}:stderr:-- >:{proc_run.stderr}\")\n",
      "                proc_run.check_returncode() # to throw the error in case there was one\n",
      "                \n",
      "            except subprocess.CalledProcessError as e:\n"
     ]
    }
   ],
   "source": [
    "# This is the code snippet which is responsible to load the model from S3\n",
    "! sed -n '40,60p' code_bloom176/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which loads the libraries into the container needed for run\n",
    "! sed -n '1,3p' code_bloom176/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which shows the environment variables being used to customize runtime\n",
    "! sed -n '1,3p' code_bloom176/serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192491d",
   "metadata": {},
   "source": [
    "### This step can take ~ 15 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f332e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742e9da",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text (specified in the 'input' field in the json ) as a prompt and Model will complete the sentence and return the results. More details on these parameters can be found at https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task. Some quick explainations are below\n",
    "1. temperature -- > The temperature of the sampling operation. 1 means regular sampling, 0 means always take the highest score and 100 means uniform probability\n",
    "2. max_new_tokens -- > The amount of new tokens or text to be generated. More tokens will increase the prediction time\n",
    "3. num_beams -- > Beam Search keeps track of the n-th most likely word sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ac534657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 ms, sys: 0 ns, total: 21.1 ms\n",
      "Wall time: 11.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\\n  \"Amazon.com is the best  online shopping site in the world. It has a wide range of products. You can buy anything you want from the site. The site is very easy to use and you can search for the product you are looking for. There are a lot of options to choose from and the prices are very reasonable. I have been using this site for a long time now and I am very happy with the service. They have a very good customer service and they are always ready to help you with any problem you have\"\\n]'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"input\": \"Amazon.com is the best \",\n",
    "            \"gen_kwargs\": {\n",
    "                \"min_length\": 5,\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.8,\n",
    "                \"num_beams\": 5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ee27a",
   "metadata": {},
   "source": [
    "#### Batch tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2fe65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"input\": [\"Amazon.com is the best \", \"DJL is the best serving model\", \"deepspeed works best\",],\n",
    "            \"gen_kwargs\": {\n",
    "                \"min_length\": 5,\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.8,\n",
    "                \"num_beams\": 5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"padding\":'True',\n",
    "                #\"padding\" : True,\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22168e",
   "metadata": {},
   "source": [
    "#### Time test\n",
    "max_new_tokens is the key for inference time since this is a text generation model\n",
    "\n",
    "max_new_tokens -- 100 leads to ~ 11 sec \n",
    "max_new_tokens -- 50 leads to ~ 5 secs\n",
    "\n",
    "max_new_tokens -- 10 leads to ~ 1 sec  -- so fairly linear response time\n",
    "\n",
    "now with batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"input\": [\"Amazon.com is the best \", \"DJL is the best serving model\", \"deepspeed works best\",], #\"Amazon.com is the best \",\n",
    "                \"gen_kwargs\": {\n",
    "                    \"min_length\": 5,\n",
    "                    \"max_new_tokens\": 50, # 100\n",
    "                    \"temperature\": 0.8, # 10 # --  0 \n",
    "                    \"num_beams\": 5,\n",
    "                    \"no_repeat_ngram_size\": 2,\n",
    "                    'padding': 'True',\n",
    "                },\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )[\"Body\"].read().decode(\"utf8\")\n",
    "    results.append((time.time() - start) * 1000)\n",
    "    \n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "07b7d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for model latency: \n",
      "\n",
      "\n",
      "P95: 6168.650722503662 ms\n",
      "\n",
      "P90: 6163.633060455322 ms\n",
      "\n",
      "Average: 6126.430702209473 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9cdd8c",
   "metadata": {},
   "source": [
    "#### Batch Tests max tokens is 50\n",
    "\n",
    "1. Run with 10 batch size -- run 5, 10, 15, 20, 25 input prompts -- Store in file 10_batch values like 5, p95 in ms for 10 runs\n",
    "2. Run with 20 batch size\n",
    "3. Run with 30 batch size\n",
    "4. Run with 40 batch size\n",
    "\n",
    "The total number of tokens produced during 1 invocation will be as follows\n",
    "\n",
    "1. Max_tokens x no of inputs (prompt_size) -- gives total number of tokens\n",
    "2. Divide by the Total time in seconds or p95\n",
    "3. This gives us the Throughput - or WALL Time for total number of tokens\n",
    "4. For tokens per second -- divide by the BATCH size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts=[\n",
    "    \"Amazon.com is the best \", \n",
    "    \"DJL is the best serving model\", \n",
    "    \"deepspeed works best for Large models\",\n",
    "    \"Large models in machine learning\",\n",
    "    \"performance bench mark tests for machine learning\"\n",
    "]*40\n",
    "print(len(input_prompts))\n",
    "input_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c49b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p temp-data/llm-perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1675c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "11\n",
      "16\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "for prompt_size in range(1,25,5):\n",
    "    print(prompt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0ad4a049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon.com is the best ',\n",
       " 'DJL is the best serving model',\n",
       " 'deepspeed works best for Large models',\n",
       " 'Large models in machine learning',\n",
       " 'performance bench mark tests for machine learning',\n",
       " 'Amazon.com is the best ',\n",
       " 'DJL is the best serving model',\n",
       " 'deepspeed works best for Large models',\n",
       " 'Large models in machine learning',\n",
       " 'performance bench mark tests for machine learning',\n",
       " 'Amazon.com is the best ',\n",
       " 'DJL is the best serving model',\n",
       " 'deepspeed works best for Large models',\n",
       " 'Large models in machine learning',\n",
       " 'performance bench mark tests for machine learning',\n",
       " 'Amazon.com is the best ',\n",
       " 'DJL is the best serving model',\n",
       " 'deepspeed works best for Large models',\n",
       " 'Large models in machine learning',\n",
       " 'performance bench mark tests for machine learning']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompts[:prompt_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt_size = 10\n",
    "batch_size = 50\n",
    "max_new_tokens=50\n",
    "\n",
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"input\": input_prompts[:prompt_size],\n",
    "            \"gen_kwargs\": {\n",
    "                \"min_length\": 5,\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 0.8,\n",
    "                \"num_beams\": 5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"padding\":'True',\n",
    "                #\"padding\" : True,\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12c9e0",
   "metadata": {},
   "source": [
    "#### Test the prompt for question answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "prompt =\n",
    "\"\"\"\n",
    "Please parse the product into words by white space. First word should be the main concept and the main concept should be as short as possible. main concept should be consistent with the category. category: product: smoked turkey breast output: turkey breast; smoked. Explanation: turkey is the main concept. smoked is a way of cooking category: food->pantry->pasta->spaghetti pasta product: whole wheat thin spaghetti box output: spaghetti; whole wheat, thin, box. Explanation: spaghetti is the main concept. whole wheat is a nutrition fact. thin is a shape. box is a packaging method. category: food->fresh produce->fresh vegetables->root vegetables->potatoes->sweet potatoes product: sweet potatoes output: sweet potatoes;. Explanation: sweet potatoes is the main concept. There is no attribute. category: food->frozen food->frozen desserts->ice creams->ice creams product: premium mint chocolate chip frozen dessert output: ice cream; premium, mint, chocolate chip, frozen, dessert. Explanation: ice cream is the main concept. ice cream is not in the product name but is implied by its category. premium is a quality. mint is a flavor. chocolate chip is a flavor. frozen is a state. category: home & garden->home->bed product: 15\" cotton voile california king bed skirt in ivory output: bed skirt; 15\", cotton, voile, california, king, ivory. Explanation: bed skirt is the main concept. 15\" is a size. cotton is a material. king is a size. ivory is a color. category: pet->pet food product: organic avocado output: avocado; organic. \n",
    "Explanation: avocado is the main concept. avocado is not a pet food. the category information is incorrect. organic is a quality. category: {c} product: {p} output:\n",
    "\"\"\"\n",
    "input_prompt = prompt.format(c=\"home improvement->bathroom->bathroom hardware->towel bar\", p=\"stainless steel towel bar\")\n",
    "\n",
    "\n",
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"input\": input_prompt,\n",
    "            \"gen_kwargs\": {\n",
    "                \"min_length\": 5,\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 0, #0.8,\n",
    "                \"num_beams\": 5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"padding\":'True',\n",
    "                'do_sample': False,\n",
    "                #\"padding\" : True,\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ba4dc002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 ms, sys: 0 ns, total: 40.7 ms\n",
      "Wall time: 437 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt_size = 5\n",
    "batch_size = 10\n",
    "max_new_tokens=50\n",
    "\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "\n",
    "prompt_size_result = []\n",
    "\n",
    "for prompt_size in range(1,42, 5): # is the index in the list of prompts\n",
    "    results = [0]\n",
    "    error_count = 0\n",
    "    total_runs = 0\n",
    "    for i in range(0, 3):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            total_runs = total_runs+1\n",
    "            smr_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=json.dumps(\n",
    "                    {\n",
    "                        \"input\": input_prompts[:prompt_size],\n",
    "                        \"gen_kwargs\": {\n",
    "                            \"min_length\": 5,\n",
    "                            \"max_new_tokens\": 50, # 100\n",
    "                            \"temperature\": 0.8, # 10 # --  0 \n",
    "                            \"num_beams\": 5,\n",
    "                            \"no_repeat_ngram_size\": 2,\n",
    "                            'padding': 'True',\n",
    "                        },\n",
    "                    }\n",
    "                ),\n",
    "                ContentType=\"application/json\",\n",
    "            )[\"Body\"].read().decode(\"utf8\")\n",
    "            results.append((time.time() - start) * 1000)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            error_count = error_count+1\n",
    "            \n",
    "    p_95_ms = str(np.percentile(results, 95)) + \" ms\"   \n",
    "    total_tokens = prompt_size * max_new_tokens\n",
    "    p_95_response_ms = np.percentile(results, 95)\n",
    "    if p_95_response_ms <= 0:\n",
    "        p_95_response_ms = 1\n",
    "    tokens_per_sec = total_tokens * 1000 / p_95_response_ms # -- since this is in ms response\n",
    "    \n",
    "    prompt_size_result.append(f\"Total_invocation={total_runs}:NoOfInputs={prompt_size}:P-95={p_95_ms}:total_tokens={total_tokens}:tokens_per_sec={tokens_per_sec}:error_count={error_count}:\\n\")\n",
    "\n",
    "with open(f\"./temp-data/llm-perf/{batch_size}_batch.txt\",\"w+\") as f:\n",
    "        f.writelines(prompt_size_result)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c07e041",
   "metadata": {},
   "source": [
    "Total_invocation=1:NoOfInputs=1:P-95=5582.748734951019 ms:total_tokens=50:tokens_per_sec=8.956161628226795:error_count=0:\n",
    "Total_invocation=1:NoOfInputs=6:P-95=6335.7338309288025 ms:total_tokens=300:tokens_per_sec=47.350473994899616:error_count=0:\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e64b2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this post, we demonstrated how to use SageMaker large model inference containers to host two large language models, BLOOM-176B and OPT-30B. We used DeepSpeed’s model parallel techniques with multiple GPUs on a single SageMaker machine learning instance. For more details about Amazon SageMaker and its large model inference capabilities, refer to the following:\n",
    "\n",
    "* Amazon SageMaker now supports deploying large models through configurable volume size and timeout quotas (https://aws.amazon.com/about-aws/whats-new/2022/09/amazon-sagemaker-deploying-large-models-volume-size-timeout-quotas/)\n",
    "* Real-time inference – Amazon SageMake (https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770b927",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "968ae9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'b2f89942-1183-4615-bce3-d0ffea0608c1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b2f89942-1183-4615-bce3-d0ffea0608c1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 06 Dec 2022 03:38:23 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "4a867c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd46ac0b7-42c2-4908-8c4a-edcef18d663e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd46ac0b7-42c2-4908-8c4a-edcef18d663e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 06 Dec 2022 03:38:24 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ed117",
   "metadata": {},
   "source": [
    "#### Optionally delete the model checkpoint from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5491e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c46e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76723e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s3_client.list_objects(Bucket=bucket, Prefix=f\"{s3_model_prefix}/\")[\"Contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea19558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
