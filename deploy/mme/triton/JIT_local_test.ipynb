{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8413 sha256=fd18f46b9edaa43cd9e4019b416b8811be26d3e8c26e8bc984f8cd1d9a3b0498\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_9_ksvtd/wheels/65/cd/01/fb75031f3f86f9d8940f46f7c23fc1dcd38965799131e06a7e\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[http] in /opt/conda/lib/python3.7/site-packages (2.27.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (1.9)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (1.21.6)\n",
      "Requirement already satisfied: geventhttpclient<=2.0.2,>=1.4.4 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (2.0.2)\n",
      "Requirement already satisfied: aiohttp>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (3.8.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.8.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (4.4.0)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (1.0.9)\n",
      "Requirement already satisfied: gevent>=0.13 in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (1.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (2022.9.24)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (1.14.0)\n",
      "Requirement already satisfied: greenlet>=0.4.14 in /opt/conda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (0.4.15)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[http]) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To create the JIT Trace model you need\n",
    "\n",
    "Torch version 1.12.1 Torch Vision 0.13.1 and Cuda library 11.3\n",
    "\n",
    "Secondly you need a GPU instance to run the notebook - this has been tested on a ml.g4dn.xlarge which comes with 1 gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com, https://download.pytorch.org/whl/cu113\n",
      "Collecting torch==1.12.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (1837.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (23.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1+cu113) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (9.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1+cu113) (1.21.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1+cu113) (2.0.4)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.12.1+cu113 torchvision-0.13.1+cu113\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tarfile\n",
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import concurrent.futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL -- Create a JIT Traced model\n",
    "\n",
    "#### Few points to note: The traced models is provided in the zip file which can be used as is\n",
    "1. The Model after trace is now returing outputs like OUTPUT_0 , 1 etc\n",
    "2. To change them to named outputs can be done and we can try post this issue gets resolved\n",
    "3. To full create a jit traced model we will need to provide a sample inputs and hence for now we have created a scripted model\n",
    "4. The TORCH and the TORCHSCRIPT libraries would need to match the container and hence we use the specific ones mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n",
      "0.13.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the model into Serving mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:709: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  \" silence this warning)\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "original_model_path = Path(\"jit-resnet-v3-model/1/model.pt\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(original_model_path)\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting jit-resnet-v3-model/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile jit-resnet-v3-model/config.pbtxt\n",
    "name: \"jit-resnet-v3-model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 12\n",
    "input {\n",
    "  name: \"INPUT__0\"\n",
    "  data_type: TYPE_UINT8\n",
    "  dims: [3,480,856]\n",
    "}\n",
    "output {\n",
    "  name: \"OUTPUT__0\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [-1]\n",
    "}\n",
    "output {\n",
    "  name: \"OUTPUT__1\"\n",
    "  data_type: TYPE_INT64\n",
    "  dims: [-1]\n",
    "}\n",
    "\n",
    "output {\n",
    "  name: \"OUTPUT__2\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: [-1]\n",
    "}\n",
    "\n",
    "instance_group {\n",
    "  count: 3\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create the tar ball and upload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import tritonclient.http as httpclient\n",
    "from botocore.config import Config\n",
    "import numpy as np\n",
    "import random\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "w,h = 856,480\n",
    "\n",
    "urls = [\n",
    "    \"https://m.media-amazon.com/images/M/MV5BNDcwZDc2NTEtMzU0Ni00YTQyLWIyYTQtNTI3YjM0MzhmMmI4XkEyXkFqcGdeQXVyNTgyNTA4MjM@._V1_.jpg\",\n",
    "    \"https://lh3.googleusercontent.com/05JfZ1ZdyzrRNvhJosUFdcjjJRFE7k2KhmeM2ujqeCbrcrCb1hkq7O_JdUBpQ3r9hi0YeSn4WgmKx3Ai8LHdM2SucxSzl9TRZ4fCAqETJ6WtHgE=s0\",\n",
    "    \"https://assets.nintendo.com/image/upload/f_auto/q_auto/dpr_2.625/c_scale,w_400/ncom/en_US/games/switch/n/new-pokemon-snap-switch/hero\",\n",
    "    \"https://images.nintendolife.com/d358c9f9118af/pokemon-go.900x.jpg\",\n",
    "    \"https://cdn.vox-cdn.com/thumbor/IKt535q8LMnJDddmLL74TBtzv88=/0x266:1024x949/1280x854/cdn.vox-cdn.com/uploads/chorus_image/image/48942277/N3DS_PokemonSuperMysteryDungeon_MainIllustration_png_jpgcopy.0.0.jpg\",\n",
    "    \"https://i.imgflip.com/3sn9mp.jpg\",\n",
    "    \"https://techcrunch.com/wp-content/uploads/2017/08/cbsn.png\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#endpoint_name = m_name\n",
    "def read_image(i=0):\n",
    "    url = random.choice(urls)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    img = img.resize((w, h), Image.ANTIALIAS)\n",
    "    return np.asarray(img, dtype='uint8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(images,endpoint_name, target_model='model.tar.gz'): # - resnet_fpn_v3.tar.gz\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_data = images # np.asarray(images, dtype='uint8')  # passing in a numpy aray already \n",
    "    print(input_data.shape)\n",
    "    \n",
    "    #inputs.append(httpclient.InferInput(\"INPUT__0\", [ len(input_data),h, w,3], \"UINT8\"))\n",
    "    \n",
    "    inputs = [httpclient.InferInput(\"INPUT__0\", images.shape, \"UINT8\")]\n",
    "    inputs[0].set_data_from_numpy(images, binary_data=True)\n",
    "    outputs = [httpclient.InferRequestedOutput(f\"OUTPUT__{n}\", binary_data=True) for n in range(3)]\n",
    "    \n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "\n",
    "\n",
    "    runtime_sm_client = boto3.client(\n",
    "        \"sagemaker-runtime\",\n",
    "        region_name=\"eu-west-1\", \n",
    "        config=Config(\n",
    "            connect_timeout=5,\n",
    "            read_timeout=60, #120,\n",
    "            retries={'max_attempts': 2,'mode': 'standard'} #20\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "            header_length\n",
    "        ),\n",
    "        Body=request_body,\n",
    "        TargetModel=target_model,\n",
    "        \n",
    "    )\n",
    "    header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "    header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "    if not header_length_str:\n",
    "        header_length_str='0'\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(response[\"Body\"].read())\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "samples = 1\n",
    "buffer = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    images_future = [executor.submit(read_image, i) for i in range(samples*batch_size)]\n",
    "\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(images_future)):\n",
    "        buffer.append(future.result())\n",
    "\n",
    "print(len(buffer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "(12, 3, 480, 856)\n"
     ]
    }
   ],
   "source": [
    "image_batch = np.asarray(buffer, dtype='uint8').transpose(0,3,1,2)  # -- 12 x 3 x 480 x 856 \n",
    "print(len(buffer[0]))\n",
    "print(image_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3, 480, 856])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(local_device)\n",
    "image_array_tensors = torch.tensor(image_batch,device=local_device)\n",
    "image_array_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:1130: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at  ../torch/csrc/jit/codegen/cuda/parser.cpp:3513.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torchvision/models/detection/retinanet.py\", line 352, in forward\n      features = x[_113]\n      conv = self.conv\n      bbox_regression = (conv).forward(features, )\n                         ~~~~~~~~~~~~~ <--- HERE\n      bbox_reg = self.bbox_reg\n      bbox_regression0 = (bbox_reg).forward(bbox_regression, )\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_42.py\", line 19, in forward\n    input1 = (_1).forward(input0, )\n    input2 = (_2).forward(input1, )\n    return (_3).forward(input2, )\n            ~~~~~~~~~~~ <--- HERE\n  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_42.Sequential) -> int:\n    return 4\n  File \"code/__torch__/torchvision/ops/misc/___torch_mangle_41.py\", line 13, in forward\n    _0 = getattr(self, \"0\")\n    _1 = getattr(self, \"1\")\n    input0 = (_0).forward(input, )\n              ~~~~~~~~~~~ <--- HERE\n    return (_1).forward(input0, )\n  def __len__(self: __torch__.torchvision.ops.misc.___torch_mangle_41.Conv2dNormActivation) -> int:\n  File \"code/__torch__/torch/nn/modules/conv/___torch_mangle_23.py\", line 23, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = (self)._conv_forward(input, weight, bias, )\n          ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return _0\n  def _conv_forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_23.Conv2d,\n  File \"code/__torch__/torch/nn/modules/conv/___torch_mangle_23.py\", line 29, in _conv_forward\n    weight: Tensor,\n    bias: Optional[Tensor]) -> Tensor:\n    _1 = torch.conv2d(input, weight, bias, [1, 1], [1, 1], [1, 1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _1\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/retinanet.py\", line 311, in forward\n    \n        for features in x:\n            bbox_regression = self.conv(features)\n                              ~~~~~~~~~ <--- HERE\n            bbox_regression = self.bbox_reg(bbox_regression)\n    \n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 457, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 453, in _conv_forward\n                            weight, bias, self.stride,\n                            _pair(0), self.dilation, self.groups)\n        return F.conv2d(input, weight, bias, self.stride,\n               ~~~~~~~~ <--- HERE\n                        self.padding, self.dilation, self.groups)\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 9.08 GiB already allocated; 6.75 MiB free; 11.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c10ce8b3491b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mst_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#result = invoke_endpoint(buffer, m_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test finished for 1 batch of {batch_size} images::result={result}::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/torchvision/models/detection/retinanet.py\", line 352, in forward\n      features = x[_113]\n      conv = self.conv\n      bbox_regression = (conv).forward(features, )\n                         ~~~~~~~~~~~~~ <--- HERE\n      bbox_reg = self.bbox_reg\n      bbox_regression0 = (bbox_reg).forward(bbox_regression, )\n  File \"code/__torch__/torch/nn/modules/container/___torch_mangle_42.py\", line 19, in forward\n    input1 = (_1).forward(input0, )\n    input2 = (_2).forward(input1, )\n    return (_3).forward(input2, )\n            ~~~~~~~~~~~ <--- HERE\n  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_42.Sequential) -> int:\n    return 4\n  File \"code/__torch__/torchvision/ops/misc/___torch_mangle_41.py\", line 13, in forward\n    _0 = getattr(self, \"0\")\n    _1 = getattr(self, \"1\")\n    input0 = (_0).forward(input, )\n              ~~~~~~~~~~~ <--- HERE\n    return (_1).forward(input0, )\n  def __len__(self: __torch__.torchvision.ops.misc.___torch_mangle_41.Conv2dNormActivation) -> int:\n  File \"code/__torch__/torch/nn/modules/conv/___torch_mangle_23.py\", line 23, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = (self)._conv_forward(input, weight, bias, )\n          ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return _0\n  def _conv_forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_23.Conv2d,\n  File \"code/__torch__/torch/nn/modules/conv/___torch_mangle_23.py\", line 29, in _conv_forward\n    weight: Tensor,\n    bias: Optional[Tensor]) -> Tensor:\n    _1 = torch.conv2d(input, weight, bias, [1, 1], [1, 1], [1, 1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _1\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/retinanet.py\", line 311, in forward\n    \n        for features in x:\n            bbox_regression = self.conv(features)\n                              ~~~~~~~~~ <--- HERE\n            bbox_regression = self.bbox_reg(bbox_regression)\n    \n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 139, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 457, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return self._conv_forward(input, self.weight, self.bias)\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 453, in _conv_forward\n                            weight, bias, self.stride,\n                            _pair(0), self.dilation, self.groups)\n        return F.conv2d(input, weight, bias, self.stride,\n               ~~~~~~~~ <--- HERE\n                        self.padding, self.dilation, self.groups)\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 9.08 GiB already allocated; 6.75 MiB free; 11.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "print(len(buffer[0]))\n",
    "st_time = time.time()\n",
    "#result = invoke_endpoint(buffer, m_name)\n",
    "result = model(image_array_tensors)\n",
    "print(len(buffer),time.time() - st_time)\n",
    "print(f\"Test finished for 1 batch of {batch_size} images::result={result}::\")\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each:output:{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [12, 0, 4], 'parameters': {'binary_data_size': 0}}\n",
      "Values:output:[] \n",
      "\n",
      "Each:output:{'name': 'OUTPUT__1', 'datatype': 'INT64', 'shape': [12, 0], 'parameters': {'binary_data_size': 0}}\n",
      "Values:output:[] \n",
      "\n",
      "Each:output:{'name': 'OUTPUT__2', 'datatype': 'FP32', 'shape': [12, 0], 'parameters': {'binary_data_size': 0}}\n",
      "Values:output:[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_outputs = result.get_response()['outputs']\n",
    "for single_output in result_outputs:\n",
    "    print(f\"Each:output:{single_output}\")\n",
    "    print(f\"Values:output:{result.as_numpy(single_output['name'])} \\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting invocation for model:: please wait ...\n",
      "\n",
      "Predictions for model latency: \n",
      "\n",
      "\n",
      "P95: 1914.787781238556 ms\n",
      "\n",
      "P90: 1897.0021724700928 ms\n",
      "\n",
      "Average: 1874.2761421203613 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Starting invocation for model:: please wait ...\")\n",
    "results = []\n",
    "for i in range(0, 50):\n",
    "    start = time.time()\n",
    "    invoke_endpoint(buffer, m_name)\n",
    "    results.append((time.time() - start) * 1000)\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for model latency: \n",
      "\n",
      "\n",
      "P95: 1914.787781238556 ms\n",
      "\n",
      "P90: 1897.0021724700928 ms\n",
      "\n",
      "Average: 1874.2761421203613 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    images = read_image()\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_data = np.asarray(images, dtype='uint8')\n",
    "    inputs.append(httpclient.InferInput(\"INPUT__0\", [ len(input_data),h, w,3], \"UINT8\"))\n",
    "    inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "    outputs.append(httpclient.InferRequestedOutput(\"BBOX\", binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(\"LABELS\", binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(\"SCORES\", binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "\n",
    "\n",
    "    runtime_sm_client = boto3.client(\"sagemaker-runtime\",region_name=\"eu-west-1\", config=Config(connect_timeout=5,\n",
    "                                                                                 read_timeout=120,\n",
    "                                                                                 retries={\n",
    "                                                                                     'max_attempts': 20,\n",
    "                                                                                     'mode': 'standard'\n",
    "\n",
    "                                                                                 }))\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "            header_length\n",
    "        ),\n",
    "        Body=request_body,\n",
    "        \n",
    "    )\n",
    "    header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "    header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(\n",
    "        response[\"Body\"].read(), \n",
    "        header_length=int(header_length_str)\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
