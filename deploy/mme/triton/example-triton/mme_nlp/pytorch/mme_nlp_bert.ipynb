{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65158321",
   "metadata": {},
   "source": [
    "# # Run mulitple NLP models on GPU with SageMaker Multi-model endpoints\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "SageMaker Multi-Model Endpoint with GPU support is a beta feature and is not recommended for production use cases\n",
    "</div>\n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) helps data scientists and developers prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML. SageMaker accelerates innovation within your organization by providing purpose-built tools for every step of ML development, including labeling, data preparation, feature engineering, statistical bias detection, AutoML, training, tuning, hosting, explainability, monitoring, and workflow automation.\n",
    "\n",
    "Customers are training ML models to cater individual users, granular market segments, hyper personalized content etc. For example, a call center analytics application using NLP language translation service to serve customers from different geographic location train custom models for different languages. Building large number of custom models can increase the cost of inference and managing models. These challenges become more pronounced when not all models are accessed at the same rate but still need to be available at all times.\n",
    "\n",
    "\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g4dn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bc886",
   "metadata": {},
   "source": [
    "## SageMaker Multi-Model Endpoints with GPU\n",
    "\n",
    "SageMaker multi-model endpoints(MME) provide a scalable and cost-effective way to deploy large numbers of ML models in the cloud. SageMaker multi-model endpoints enable you to deploy multiple ML models behind a single endpoint and serve them using a single serving container. Today, customers can use MME on CPU based instance types limiting them to deploy deep learning models that need accelerated compute GPUs. With announcement of new private beta feature, customer can host and serve deep learning GPU models using SageMaker multi-model endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556df546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(\"./images/mme-gpu.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631873a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8076c442",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "SageMaker MME with GPU works with NVIDIA Triton server\n",
    "\n",
    "[NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/) was developed specifically to enable scalable, cost-effective, and easy deployment of models in production. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance.\n",
    "\n",
    "Some key features of Triton are:\n",
    "* **Support for Multiple frameworks**: Triton can be used to deploy models from all major frameworks. Triton supports TensorFlow GraphDef, TensorFlow SavedModel, ONNX, PyTorch TorchScript, TensorRT, RAPIDS FIL for tree based models, and OpenVINO model formats. \n",
    "* **Model pipelines**: Triton model ensemble represents a pipeline of one or more models or pre/post processing logic and the connection of input and output tensors between them. A single inference request to an ensemble will trigger the execution of the entire pipeline.\n",
    "* **Concurrent model execution**: Multiple models (or multiple instances of the same model) can run simultaneously on the same GPU or on multiple GPUs for different model management needs.\n",
    "* **Dynamic batching**: For models that support batching, Triton has multiple built-in scheduling and batching algorithms that combine individual inference requests together to improve inference throughput. These scheduling and batching decisions are transparent to the client requesting inference.\n",
    "* **Diverse CPUs and GPUs**: The models can be executed on CPUs or GPUs for maximum flexibility and to support heterogeneous computing requirements.\n",
    "\n",
    "In this notebook, we will use deploy multiple `bert-large-uncased` models to SageMaker multi model endpoint behind `g4dn.4xlarge` instance type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ac826",
   "metadata": {},
   "source": [
    "### Installs\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server. Update SageMaker, boto3, awscli etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3a6079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.27.67 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8413 sha256=1273196bffda8d0fe101accbb2386a5f2429062e8811703a5121b57801e2b0c4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e0/c2/fb/5cf4e1cfaf28007238362cb746fb38fc2dd76348331a748d54\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Collecting tritonclient[http]\n",
      "  Downloading tritonclient-2.25.0-py3-none-manylinux1_x86_64.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m331.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tritonclient[http]) (1.20.3)\n",
      "Collecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m359.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting geventhttpclient>=1.4.4\n",
      "  Downloading geventhttpclient-2.0.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m310.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tritonclient[http]) (3.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (2.0.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (1.16.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (21.8.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (2021.10.8)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m359.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp>=3.8.1->tritonclient[http]) (4.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (59.4.0)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (5.4.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (1.1.2)\n",
      "Requirement already satisfied: zope.event in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (4.5.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[http]) (3.1)\n",
      "Installing collected packages: brotli, python-rapidjson, tritonclient, geventhttpclient\n",
      "Successfully installed brotli-1.0.9 geventhttpclient-2.0.2 python-rapidjson-1.8 tritonclient-2.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker transformers==4.9.1\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bca17",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbacec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# triton\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61f8d9",
   "metadata": {},
   "source": [
    "#### Set Variables\n",
    "\n",
    "We set SageMaker variables and other variables below, also define the IAM role that will give Amazon SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c09610",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"bert_mme_gpu\"\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "cw_client = boto3.client(\"cloudwatch\", region)\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region Name: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = \"bert-mme-gpu-\" + ts\n",
    "print(f\"SageMaker Model Name: {sm_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c57df6",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker Triton Inference Server Deep Learning Container Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5d6ed",
   "metadata": {},
   "source": [
    "Set `mme_triton_image_uri` based on the `account_id` and `region` information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b64a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ap-south-1 region\n",
    "mme_triton_image_uri = \"850464037171.dkr.ecr.ap-south-1.amazonaws.com/tritonserver:22.07-py3\"\n",
    "#us-east-1 region\n",
    "#mme_triton_image_uri = \"785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656a690",
   "metadata": {},
   "source": [
    "## NVIDIA Triton Setup with Amazon SageMaker\n",
    "\n",
    "1. We will use this [generate_models.sh](./workspace/generate_models.sh) to generate the `bert-large-uncased` model to be used with NVIDIA Triton inference server.\n",
    "2. The script for loading the pre-trained `bert_large_uncased` model and saving it can be found in this [pt_exporter.py](./workspace/pt_exporter.py)\n",
    "3. Pre-trained model is loaded in torchscript format and model artifacts are jit traced with a dummy input and store in model.pt format\n",
    "4. After the model is serialized we package it into the format that Triton and SageMaker expect it to be.\n",
    "5. We used the pre-configured `config.pbtxt` file provided with this repo to specify model [configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load the model. \n",
    "6. We tar the model directory and upload it to s3 to later create a [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23f1fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> NOTE </strong>\n",
    "The below script uses docker and thus will not work on Amazon SageMaker Studio notebook. Please use Amazon SageMaker Notebook instance to execute this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365439c",
   "metadata": {},
   "source": [
    "### Workflow overview\n",
    "\n",
    "#### 1a. PyTorch model\n",
    "In this step, we load pre-trained ResNet50 model from torch and save as `model.pt` file. We use torch.jit.script to compile the code as TorchScript code using TorchScript compiler. It needs an example inputs, so we pass 1 instance of a RGB image(3X224X224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:22.07-py3 \\\n",
    "            /bin/bash generate_model_pytorch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc673e",
   "metadata": {},
   "source": [
    "The script saves the model in this [workspace](./workspace/) directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2df460",
   "metadata": {},
   "source": [
    "#### 2. Build Model Respository\n",
    "\n",
    "We used the pre-configured `config.pbtxt` file provided with this repo to specify model [configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load the model.\n",
    "\n",
    "The model repository contains model to serve, in our case it will be the model.plan and configuration file with input/output specifications and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddf403",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bert\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.pt\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558572b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1cdc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile triton-serve/bert-uc/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bff5a",
   "metadata": {},
   "source": [
    "#### 3. Export model artifacts to S3\n",
    "\n",
    "SageMaker expects the model artifacts in below format, it should also satisfy Triton container requirements such as model name, version, config.pbtxt files etc. `tar` the folder containing the model file as `model.tar.gz` and upload it to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e43a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/bert/1/\n",
    "!mv -f workspace/model.pt triton-serve-pt/bert/1/\n",
    "!tar -C triton-serve-pt/ -czf bert_pt_v0.tar.gz bert\n",
    "model_uri_pt = sagemaker_session.upload_data(path=\"bert_pt_v0.tar.gz\", key_prefix=\"bert_mme_gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186a517",
   "metadata": {},
   "source": [
    "#### 4. Creating copies of model to be loaded to MME\n",
    "\n",
    "We will create 4 copies of the `bert-large-uncased` model to be used with SageMaker multi-model endpoint(MME). In practice, this could be 1000s of custom model depending on ML application and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36303cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$prefix/bert_pt_v0.tar.gz s3://$bucket/$prefix/bert_pt_v1.tar.gz\n",
    "!aws s3 cp s3://$bucket/$prefix/bert_pt_v0.tar.gz s3://$bucket/$prefix/bert_pt_v2.tar.gz\n",
    "!aws s3 cp s3://$bucket/$prefix/bert_pt_v0.tar.gz s3://$bucket/$prefix/bert_pt_v3.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860c89a",
   "metadata": {},
   "source": [
    "#### 5. Create SageMaker Endpoint\n",
    "\n",
    "Now that we have 4 models, we start off by creating a [sagemaker model](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3**. This variable is optional in case of a single model. In case of ensemble models, this key **has to be** specified for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b8a56",
   "metadata": {},
   "source": [
    "`model_data_url` is the S3 directory that contains all the models that SageMaker mulit-model endpoint will use to load  and serve predictions. `Mode` indicated the mode in which SageMaker would host this model - `MultiModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1951634",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$bucket/$prefix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"bert\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50763a0a",
   "metadata": {},
   "source": [
    "Once the image, data location are set we create the model using `create_model` by specifying the `ModelName` and the Container definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"bert-mme-gpu-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54ae46",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"bert-mme-gpu-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da0741",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb30477",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"bert-mme-gpu-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41547b",
   "metadata": {},
   "source": [
    "#### 7. Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e044f",
   "metadata": {},
   "source": [
    "####  Create payload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155d6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    enc = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e3cce",
   "metadata": {},
   "source": [
    "If you want to change the payload (Token Length), below are the changes -\n",
    "1. Change the JSON with shape reflecting the right token length below\n",
    "2. Change the tokenize_text method to reflect the token length\n",
    "3. Change the config.pbtxt the triton* folder to reflect the input id and attention mask length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b0137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b679cca7e14141b691c3a313b52cada0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23eb4417cc574206b745bea46f5611d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5af7838df1649ecad595f0ec944e2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e7297aa33848ae9c34bd4deb5997c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_triton = \"\"\"\n",
    "                Create payload JSON and upload it on S3. \n",
    "                This will be used by Inference Recommender to run the load test.\n",
    "              \"\"\"\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(text_triton)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, 512], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, 512], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b71d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3443, 18093, 1046, 3385, 1998, 2039, 11066, 2009, 2006, 1055, 2509, 1012, 2023, 2097, 2022, 2109, 2011, 28937, 16755, 2121, 2000, 2448, 1996, 7170, 3231, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(len(input_ids)) # should be 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8a977",
   "metadata": {},
   "source": [
    "We specify the model artifact name `TargetModel` model to request for inference when invoking a multi-model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8c017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload),\n",
    "    TargetModel='bert_pt_v0.tar.gz'\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026efa0",
   "metadata": {},
   "source": [
    "Let's invoke different model to simulate traffic to MME endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    n = random.randint(0,3)\n",
    "\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"bert_pt_v{n}.tar.gz\"\n",
    "        )\n",
    "    response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    output = response['outputs'][0]['data']\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40186b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=sm_model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfeec1c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides an overview of new private beta feature to host mulitple deep learning models with Amazon SageMaker Mulit model endpoints(MME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499645bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
