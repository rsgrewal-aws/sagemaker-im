{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 01\n",
    "%pip install -q  xgboost==1.3.1 pandas==1.0.5 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 02\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import joblib\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Setting SageMaker variables\n",
    "sess = sagemaker.Session()\n",
    "write_bucket = sess.default_bucket()\n",
    "write_prefix = \"fraud-detect-demo\"\n",
    "\n",
    "region = sess.boto_region_name\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "read_bucket = \"sagemaker-sample-files\"\n",
    "read_prefix = \"datasets/tabular/synthetic_automobile_claims\" \n",
    "\n",
    "\n",
    "# Setting S3 location for read and write operations\n",
    "train_data_key = f\"{read_prefix}/train.csv\"\n",
    "test_data_key = f\"{read_prefix}/test.csv\"\n",
    "validation_data_key = f\"{read_prefix}/validation.csv\"\n",
    "model_key = f\"{write_prefix}/model\"\n",
    "output_key = f\"{write_prefix}/output\"\n",
    "\n",
    "\n",
    "train_data_uri = f\"s3://{read_bucket}/{train_data_key}\"\n",
    "test_data_uri = f\"s3://{read_bucket}/{test_data_key}\"\n",
    "validation_data_uri = f\"s3://{read_bucket}/{validation_data_key}\"\n",
    "model_uri = f\"s3://{write_bucket}/{model_key}\"\n",
    "output_uri = f\"s3://{write_bucket}/{output_key}\"\n",
    "estimator_output_uri = f\"s3://{write_bucket}/{write_prefix}/training_jobs\"\n",
    "bias_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify-output/bias\"\n",
    "explainability_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify-output/explainability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 03\n",
    "\n",
    "tuning_job_name_prefix = \"xgbtune\" \n",
    "training_job_name_prefix = \"xgbtrain\"\n",
    "\n",
    "xgb_model_name = \"fraud-detect-xgb-model\"\n",
    "endpoint_name_prefix = \"xgb-fraud-model-dev\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.m4.xlarge\"\n",
    "clarify_instance_count = 1\n",
    "clarify_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 04\n",
    "\n",
    "!mkdir -p tune-scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tune-scripts/xgboost_train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Cell 05\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters and algorithm parameters are described here\n",
    "    parser.add_argument(\"--num_round\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--eta\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--colsample_bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--objective\", type=str, default=\"binary:logistic\")\n",
    "    parser.add_argument(\"--eval_metric\", type=str, default=\"auc\")\n",
    "    parser.add_argument(\"--nfold\", type=int, default=3)\n",
    "    parser.add_argument(\"--early_stopping_rounds\", type=int, default=3)\n",
    "    \n",
    "\n",
    "    # SageMaker specific arguments. Defaults are set in the environment variables\n",
    "    # Location of input training data\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    # Location of input validation data\n",
    "    parser.add_argument(\"--validation_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    # Location where trained model will be stored. Default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    # Location where model artifacts will be stored. Default set by SageMaker, /opt/ml/output/data\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_train = pd.read_csv(f\"{args.train_data_dir}/train.csv\")\n",
    "    train = data_train.drop(\"fraud\", axis=1)\n",
    "    label_train = pd.DataFrame(data_train[\"fraud\"])\n",
    "    dtrain = xgb.DMatrix(train, label=label_train)\n",
    "    \n",
    "    \n",
    "    data_validation = pd.read_csv(f\"{args.validation_data_dir}/validation.csv\")\n",
    "    validation = data_validation.drop(\"fraud\", axis=1)\n",
    "    label_validation = pd.DataFrame(data_validation[\"fraud\"])\n",
    "    dvalidation = xgb.DMatrix(validation, label=label_validation)\n",
    "\n",
    "    params = {\"max_depth\": args.max_depth,\n",
    "              \"eta\": args.eta,\n",
    "              \"objective\": args.objective,\n",
    "              \"subsample\" : args.subsample,\n",
    "              \"colsample_bytree\":args.colsample_bytree\n",
    "             }\n",
    "    \n",
    "    num_boost_round = args.num_round\n",
    "    nfold = args.nfold\n",
    "    early_stopping_rounds = args.early_stopping_rounds\n",
    "    \n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        metrics=[\"auc\"],\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=len(cv_results))\n",
    "    \n",
    "    train_pred = model.predict(dtrain)\n",
    "    validation_pred = model.predict(dvalidation)\n",
    "    \n",
    "    train_auc = roc_auc_score(label_train, train_pred)\n",
    "    validation_auc = roc_auc_score(label_validation, validation_pred)\n",
    "    \n",
    "    print(f\"[0]#011train-auc:{train_auc:.2f}\")\n",
    "    print(f\"[0]#011validation-auc:{validation_auc:.2f}\")\n",
    "\n",
    "    metrics_data = {\"hyperparameters\" : params,\n",
    "                    \"binary_classification_metrics\": {\"validation:auc\": {\"value\": validation_auc},\n",
    "                                                      \"train:auc\": {\"value\": train_auc}\n",
    "                                                     }\n",
    "                   }\n",
    "              \n",
    "    # Save the evaluation metrics to the location specified by output_data_dir\n",
    "    metrics_location = args.output_data_dir + \"/metrics.json\"\n",
    "    \n",
    "    # Save the model to the location specified by model_dir\n",
    "    model_location = args.model_dir + \"/xgboost-model\"\n",
    "\n",
    "    with open(metrics_location, \"w\") as f:\n",
    "        json.dump(metrics_data, f)\n",
    "\n",
    "    with open(model_location, \"wb\") as f:\n",
    "        joblib.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 06\n",
    "\n",
    "# SageMaker estimator\n",
    "\n",
    "# Set static hyperparameters that will not be tuned\n",
    "static_hyperparams = {  \n",
    "                        \"eval_metric\" : \"auc\",\n",
    "                        \"objective\": \"binary:logistic\",\n",
    "                        \"num_round\": \"5\"\n",
    "                      }\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"xgboost_train.py\",\n",
    "    source_dir=\"tune-scripts\",\n",
    "    output_path=estimator_output_uri,\n",
    "    code_location=estimator_output_uri,\n",
    "    hyperparameters=static_hyperparams,\n",
    "    role=sagemaker_role,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"1.3-1\",\n",
    "    base_job_name=training_job_name_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 07\n",
    "\n",
    "# Setting ranges of hyperparameters to be tuned\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"subsample\": ContinuousParameter(0.7, 0.95),\n",
    "    \"colsample_bytree\": ContinuousParameter(0.7, 0.95),\n",
    "    \"max_depth\": IntegerParameter(1, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 08\n",
    "\n",
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "# Setting up tuner object\n",
    "tuner_config_dict = {\n",
    "                     \"estimator\" : xgb_estimator,\n",
    "                     \"max_jobs\" : 5,\n",
    "                     \"max_parallel_jobs\" : 2,\n",
    "                     \"objective_metric_name\" : objective_metric_name,\n",
    "                     \"hyperparameter_ranges\" : hyperparameter_ranges,\n",
    "                     \"base_tuning_job_name\" : tuning_job_name_prefix,\n",
    "                     \"strategy\" : \"Random\"\n",
    "                    }\n",
    "tuner = HyperparameterTuner(**tuner_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 09\n",
    "\n",
    "# Setting the input channels for tuning job\n",
    "s3_input_train = TrainingInput(s3_data=\"s3://{}/{}\".format(read_bucket, train_data_key), content_type=\"csv\", s3_data_type=\"S3Prefix\")\n",
    "s3_input_validation = (\n",
    "    TrainingInput(s3_data=\"s3://{}/{}\".format(\n",
    "        read_bucket, validation_data_key), \n",
    "        content_type=\"csv\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    ")\n",
    "\n",
    "tuner.fit(inputs={\"train\": s3_input_train, \"validation\": s3_input_validation}, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "\n",
    "# Summary of tuning results ordered in descending order of performance\n",
    "df_tuner = sagemaker.HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.job_name).dataframe()\n",
    "df_tuner = df_tuner[df_tuner[\"FinalObjectiveValue\"]>-float('inf')].sort_values(\"FinalObjectiveValue\", ascending=False)\n",
    "df_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "\n",
    "tuner_job_info = sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)\n",
    "\n",
    "model_matches = sagemaker_client.list_models(NameContains=xgb_model_name)[\"Models\"]\n",
    "\n",
    "if not model_matches:\n",
    "    _ = sess.create_model_from_job(\n",
    "            name=xgb_model_name,\n",
    "            training_job_name=tuner_job_info['BestTrainingJob'][\"TrainingJobName\"],\n",
    "            role=sagemaker_role,\n",
    "            image_uri=tuner_job_info['TrainingJobDefinition'][\"AlgorithmSpecification\"][\"TrainingImage\"]\n",
    "            )\n",
    "else:\n",
    "\n",
    "    print(f\"Model {xgb_model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 12\n",
    "\n",
    "train_df = pd.read_csv(train_data_uri)\n",
    "train_df_cols = train_df.columns.to_list()\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=clarify_instance_count,\n",
    "    instance_type=clarify_instance_type,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Data config\n",
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_uri,\n",
    "    s3_output_path=bias_report_output_uri,\n",
    "    label=\"fraud\",\n",
    "    headers=train_df_cols,\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "# Model config\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    model_name=xgb_model_name,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "# Model predictions config to get binary labels from probabilities\n",
    "predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)\n",
    "\n",
    "# Bias config\n",
    "bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name=\"customer_gender_female\",\n",
    "    facet_values_or_threshold=[1],\n",
    ")\n",
    "\n",
    "# Run Clarify job\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=[\"CI\"],\n",
    "    post_training_methods=[\"DPPL\"])\n",
    "\n",
    "clarify_bias_job_name = clarify_processor.latest_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 13\n",
    "\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=[\"CI\"],\n",
    "    post_training_methods=[\"DPPL\"]\n",
    "    )\n",
    "\n",
    "clarify_bias_job_name = clarify_processor.latest_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 14\n",
    "\n",
    "# Copy bias report and view locally\n",
    "!aws s3 cp s3://{write_bucket}/{write_prefix}/clarify-output/bias/report.pdf ./clarify_bias_output.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 15\n",
    "\n",
    "explainability_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_uri,\n",
    "    s3_output_path=explainability_report_output_uri,\n",
    "    label=\"fraud\",\n",
    "    headers=train_df_cols,\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "# Use mean of train dataset as baseline data point\n",
    "shap_baseline = [list(train_df.drop([\"fraud\"], axis=1).mean())]\n",
    "\n",
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    baseline=shap_baseline,\n",
    "    num_samples=500,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    ")\n",
    "\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 16\n",
    "\n",
    "# Copy explainability report and view\n",
    "!aws s3 cp s3://{write_bucket}/{write_prefix}/clarify-output/explainability/report.pdf ./clarify_explainability_output.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 17\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "local_explanations_out = pd.read_csv(explainability_report_output_uri + \"/explanations_shap/out.csv\")\n",
    "feature_names = [str.replace(c, \"_label0\", \"\") for c in \n",
    "local_explanations_out.columns.to_series()]\n",
    "local_explanations_out.columns = feature_names\n",
    "\n",
    "selected_example = 100\n",
    "print(\"Example number:\", selected_example)\n",
    "\n",
    "local_explanations_out.iloc[selected_example].plot(\n",
    "    kind=\"bar\", title=\"Local explanation for the example number \" + str(selected_example), rot=60, figsize=(20, 8)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 18\n",
    "\n",
    "best_train_job_name = tuner.best_training_job()\n",
    "\n",
    "model_path = estimator_output_uri + '/' + best_train_job_name + '/output/model.tar.gz'\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")\n",
    "create_model_config = {\"model_data\":model_path,\n",
    "                       \"role\":sagemaker_role,\n",
    "                       \"image_uri\":training_image,\n",
    "                       \"name\":endpoint_name_prefix,\n",
    "                       \"predictor_cls\":sagemaker.predictor.Predictor\n",
    "                       }\n",
    "# Create a SageMaker model\n",
    "model = sagemaker.model.Model(**create_model_config)\n",
    "\n",
    "# Deploy the best model and get access to a SageMaker Predictor\n",
    "predictor = model.deploy(initial_instance_count=predictor_instance_count, \n",
    "                         instance_type=predictor_instance_type,\n",
    "                         serializer=CSVSerializer(),\n",
    "                         deserializer=CSVDeserializer())\n",
    "print(f\"\\nModel deployed at endpoint : {model.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 19\n",
    "\n",
    "# Sample test data\n",
    "test_df = pd.read_csv(test_data_uri)\n",
    "payload = test_df.drop([\"fraud\"], axis=1).iloc[0].to_list()\n",
    "print(f\"Model predicted score : {float(predictor.predict(payload)[0][0]):.3f}, True label : {test_df['fraud'].iloc[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "try:\n",
    "\tsess.delete_model(xgb_model_name)\n",
    "except:\n",
    "\tpass\n",
    "sess.delete_model(model.name)\n",
    "\n",
    "# Delete inference endpoint config\n",
    "sess.delete_endpoint_config(endpoint_config_name=predictor._get_endpoint_config_name())\n",
    "\n",
    "# Delete inference endpoint\n",
    "sess.delete_endpoint(endpoint_name=model.endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
