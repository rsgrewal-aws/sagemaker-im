{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 01\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    ScriptProcessor\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, \n",
    "    TrainingStep, \n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, \n",
    "    ParameterFloat, \n",
    "    ParameterString, \n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ModelBiasCheckConfig, \n",
    "    ClarifyCheckStep, \n",
    "    ModelExplainabilityCheckConfig\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource, \n",
    "    ModelMetrics, \n",
    "    FileSource\n",
    ")\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "from sagemaker.image_uris import retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 02\n",
    "\n",
    "# Instantiate AWS services session and client objects\n",
    "sess = sagemaker.Session()\n",
    "write_bucket = sess.default_bucket()\n",
    "write_prefix = \"fraud-detect-demo\"\n",
    "\n",
    "region = sess.boto_region_name\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Fetch SageMaker execution role\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# S3 locations used for parameterizing the notebook run\n",
    "read_bucket = \"sagemaker-sample-files\"\n",
    "read_prefix = \"datasets/tabular/synthetic_automobile_claims\" \n",
    "\n",
    "# S3 location where raw data to be fetched from\n",
    "raw_data_key = f\"s3://{read_bucket}/{read_prefix}\"\n",
    "\n",
    "# S3 location where processed data to be uploaded\n",
    "processed_data_key = f\"{write_prefix}/processed\"\n",
    "\n",
    "# S3 location where train data to be uploaded\n",
    "train_data_key = f\"{write_prefix}/train\"\n",
    "\n",
    "# S3 location where validation data to be uploaded\n",
    "validation_data_key = f\"{write_prefix}/validation\"\n",
    "\n",
    "# S3 location where test data to be uploaded\n",
    "test_data_key = f\"{write_prefix}/test\"\n",
    "\n",
    "\n",
    "# Full S3 paths\n",
    "claims_data_uri = f\"{raw_data_key}/claims.csv\"\n",
    "customers_data_uri = f\"{raw_data_key}/customers.csv\"\n",
    "output_data_uri = f\"s3://{write_bucket}/{write_prefix}/\"\n",
    "scripts_uri = f\"s3://{write_bucket}/{write_prefix}/scripts\"\n",
    "estimator_output_uri = f\"s3://{write_bucket}/{write_prefix}/training_jobs\"\n",
    "processing_output_uri = f\"s3://{write_bucket}/{write_prefix}/processing_jobs\"\n",
    "model_eval_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_eval\"\n",
    "clarify_bias_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/bias_config\"\n",
    "clarify_explainability_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/explainability_config\"\n",
    "bias_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/bias\"\n",
    "explainability_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/explainability\"\n",
    "\n",
    "# Retrieve training image\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix::2022-07-28-01-56-48::\n",
      "endpoint_name_config=fraud-detect-xgb-pipeline2022-07-28-01-56-48-endpoint-config\n",
      "endpoint_name=fraud-detect-xgb-pipeline2022-07-28-01-56-48-endpoint\n"
     ]
    }
   ],
   "source": [
    "# Cell 03\n",
    "\n",
    "# Set names of pipeline objects\n",
    "import time\n",
    "\n",
    "suffix_prefix=time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "pipeline_name = \"FraudDetectXGBPipeline\"\n",
    "pipeline_model_name = \"fraud-detect-xgb-pipeline\"+suffix_prefix\n",
    "model_package_group_name = \"fraud-detect-xgb-model-group\"\n",
    "base_job_name_prefix = \"fraud-detect\"\n",
    "endpoint_config_name = f\"{pipeline_model_name}-endpoint-config\"\n",
    "endpoint_name = f\"{pipeline_model_name}-endpoint\"\n",
    "\n",
    "# Set data parameters\n",
    "target_col = \"fraud\"\n",
    "\n",
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.c5.xlarge\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.m4.xlarge\"\n",
    "clarify_instance_count = 1\n",
    "clarify_instance_type = \"ml.m4.xlarge\"\n",
    "print(f\"suffix::{suffix_prefix}::\")\n",
    "print(f\"endpoint_name_config={endpoint_config_name}\")\n",
    "print(f\"endpoint_name={endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 04\n",
    "\n",
    "# Set up pipeline input parameters\n",
    "\n",
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance count\n",
    "train_instance_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=train_instance_count\n",
    ")\n",
    "\n",
    "# Set deployment instance type\n",
    "deploy_instance_type_param = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=predictor_instance_type,\n",
    ")\n",
    "\n",
    "# Set deployment instance count\n",
    "deploy_instance_count_param = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=predictor_instance_count\n",
    ")\n",
    "\n",
    "# Set Clarify check instance type\n",
    "clarify_instance_type_param = ParameterString(\n",
    "    name=\"ClarifyInstanceType\",\n",
    "    default_value=clarify_instance_type,\n",
    ")\n",
    "\n",
    "# Set model bias check params\n",
    "skip_check_model_bias_param = ParameterBoolean(\n",
    "    name=\"SkipModelBiasCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_bias_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelBiasBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_bias_param = ParameterString(\n",
    "    name=\"ModelBiasSuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model explainability check params\n",
    "skip_check_model_explainability_param = ParameterBoolean(\n",
    "    name=\"SkipModelExplainabilityCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_explainability_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelExplainabilityBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_explainability_param = ParameterString(\n",
    "    name=\"ModelExplainabilitySuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model approval param\n",
    "model_approval_status_param = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pipeline is a sequence of steps that can be individually built and then put together to form an ML workflow. \n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, you build a pipeline with the following steps:\n",
    "\n",
    "    Data processing step: Runs a SageMaker Processing job using the input raw data in S3 and outputs training, validation, and test splits to S3.\n",
    "    Training step: Trains an XGBoost model using SageMaker training jobs with training and validation data in S3 as inputs, and stores the trained model artifact in S3.\n",
    "    Evaluation step: Evaluates the model on the test dataset by running a SageMaker Processing job using the test data and the model artifact in S3 as inputs, and stores the output model performance evaluation report in S3.\n",
    "    Conditional step: Compares model performance on the test dataset against the threshold. Runs a SageMaker Pipelines predefined step using the model performance evaluation report in S3 as input, and stores the output list of pipeline steps that will be executed if model performance is acceptable.\n",
    "    Create model step: Runs a SageMaker Pipelines predefined step using the model artifact in S3 as an input, and stores the output SageMaker model in S3.\n",
    "    Bias check step: Checks for model bias using SageMaker Clarify with the training data and model artifact in S3 as inputs and stores the model bias report and baseline metrics in S3.\n",
    "    Model explainability step: Runs SageMaker Clarify with the training data and model artifact in S3 as inputs, and stores the model explainability report and baseline metrics in S3.\n",
    "    Register step: Runs a SageMaker Pipelines predefined step using the model, bias, and explainability baseline metrics as inputs to register the model in the SageMaker Model Registry.\n",
    "    Deploy step: Runs a SageMaker Pipelines predefined step using an AWS Lambda handler function, the model, and the endpoint configuration as inputs to deploy the model to a SageMaker Real-Time Inference endpoint.\n",
    "\n",
    "SageMaker Pipelines provides many predefined step types, such as steps for data processing, model training, model tuning, and batch transformation. For more information, see Pipeline Steps in the Amazon SageMaker Developer Guide. In the following steps, you configure and define each pipeline step individually, and then define the pipeline itself by combining the pipeline steps with the input parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 05\n",
    "!mkdir -p temp-scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp-scripts/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp-scripts/preprocessing.py\n",
    "# Cell 06\n",
    "import argparse\n",
    "import pathlib\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-ratio\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--validation-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--test-ratio\", type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    logger.info(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # Set local path prefix in the processing container\n",
    "    local_dir = \"/opt/ml/processing\"    \n",
    "    \n",
    "    input_data_path_claims = os.path.join(\"/opt/ml/processing/claims\", \"claims.csv\")\n",
    "    input_data_path_customers = os.path.join(\"/opt/ml/processing/customers\", \"customers.csv\")\n",
    "    \n",
    "    logger.info(\"Reading claims data from {}\".format(input_data_path_claims))\n",
    "    df_claims = pd.read_csv(input_data_path_claims)\n",
    "    \n",
    "    logger.info(\"Reading customers data from {}\".format(input_data_path_customers))\n",
    "    df_customers = pd.read_csv(input_data_path_customers)\n",
    "    \n",
    "    logger.debug(\"Formatting column names.\")\n",
    "    # Format column names\n",
    "    df_claims = df_claims.rename({c : c.lower().strip().replace(' ', '_') for c in df_claims.columns}, axis = 1)\n",
    "    df_customers = df_customers.rename({c : c.lower().strip().replace(' ', '_') for c in df_customers.columns}, axis = 1)\n",
    "    \n",
    "    logger.debug(\"Joining datasets.\")\n",
    "    # Join datasets\n",
    "    df_data = df_claims.merge(df_customers, on = 'policy_id', how = 'left')\n",
    "\n",
    "    # Drop selected columns not required for model building\n",
    "    df_data = df_data.drop(['customer_zip'], axis = 1)\n",
    "    \n",
    "    # Select Ordinal columns\n",
    "    ordinal_cols = [\"police_report_available\", \"policy_liability\", \"customer_education\"]\n",
    "\n",
    "    # Select categorical columns and filling with na\n",
    "    cat_cols_all = list(df_data.select_dtypes('object').columns)\n",
    "    cat_cols = [c for c in cat_cols_all if c not in ordinal_cols]\n",
    "    df_data[cat_cols] = df_data[cat_cols].fillna('na')\n",
    "    \n",
    "    logger.debug(\"One-hot encoding categorical columns.\")\n",
    "    # One-hot encoding categorical columns\n",
    "    df_data = pd.get_dummies(df_data, columns = cat_cols)\n",
    "    \n",
    "    logger.debug(\"Encoding ordinal columns.\")\n",
    "    # Ordinal encoding\n",
    "    mapping = {\n",
    "               \"Yes\": \"1\",\n",
    "               \"No\": \"0\" \n",
    "              }\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].map(mapping)\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"15/30\": \"0\",\n",
    "               \"25/50\": \"1\", \n",
    "               \"30/60\": \"2\",\n",
    "               \"100/200\": \"3\"\n",
    "              }\n",
    "    \n",
    "    df_data['policy_liability'] = df_data['policy_liability'].map(mapping)\n",
    "    df_data['policy_liability'] = df_data['policy_liability'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"Below High School\": \"0\",\n",
    "               \"High School\": \"1\", \n",
    "               \"Associate\": \"2\",\n",
    "               \"Bachelor\": \"3\",\n",
    "               \"Advanced Degree\": \"4\"\n",
    "              }\n",
    "    \n",
    "    df_data['customer_education'] = df_data['customer_education'].map(mapping)\n",
    "    df_data['customer_education'] = df_data['customer_education'].astype(float)\n",
    "    \n",
    "    df_processed = df_data.copy()\n",
    "    df_processed.columns = [c.lower() for c in df_data.columns]\n",
    "    df_processed = df_processed.drop([\"policy_id\", \"customer_gender_unkown\"], axis=1)\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_ratio = args.train_ratio\n",
    "    val_ratio = args.validation_ratio\n",
    "    test_ratio = args.test_ratio\n",
    "    \n",
    "    logger.debug(\"Splitting data into train, validation, and test sets\")\n",
    "    \n",
    "    y = df_processed['fraud']\n",
    "    X = df_processed.drop(['fraud'], axis = 1)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    train_df = pd.concat([y_train, X_train], axis = 1)\n",
    "    val_df = pd.concat([y_val, X_val], axis = 1)\n",
    "    test_df = pd.concat([y_test, X_test], axis = 1)\n",
    "    dataset_df = pd.concat([y, X], axis = 1)\n",
    "    \n",
    "    logger.info(\"Train data shape after preprocessing: {}\".format(train_df.shape))\n",
    "    logger.info(\"Validation data shape after preprocessing: {}\".format(val_df.shape))\n",
    "    logger.info(\"Test data shape after preprocessing: {}\".format(test_df.shape))\n",
    "    \n",
    "    # Save processed datasets to the local paths in the processing container.\n",
    "    # SageMaker will upload the contents of these paths to S3 bucket\n",
    "    logger.debug(\"Writing processed datasets to container local path.\")\n",
    "    train_output_path = os.path.join(f\"{local_dir}/train\", \"train.csv\")\n",
    "    validation_output_path = os.path.join(f\"{local_dir}/val\", \"validation.csv\")\n",
    "    test_output_path = os.path.join(f\"{local_dir}/test\", \"test.csv\")\n",
    "    full_processed_output_path = os.path.join(f\"{local_dir}/full\", \"dataset.csv\")\n",
    "\n",
    "    logger.info(\"Saving train data to {}\".format(train_output_path))\n",
    "    train_df.to_csv(train_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving validation data to {}\".format(validation_output_path))\n",
    "    val_df.to_csv(validation_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving test data to {}\".format(test_output_path))\n",
    "    test_df.to_csv(test_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving full processed data to {}\".format(full_processed_output_path))\n",
    "    dataset_df.to_csv(full_processed_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 07\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "# Upload processing script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"temp-scripts/preprocessing.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/preprocessing.py\"\n",
    ")\n",
    "\n",
    "# Define the SKLearnProcessor configuration\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=process_instance_type,\n",
    "    base_job_name=f\"{base_job_name_prefix}-processing\",\n",
    ")\n",
    "\n",
    "# Define pipeline processing step\n",
    "process_step = ProcessingStep(\n",
    "    name=\"DataProcessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=claims_data_uri, destination=\"/opt/ml/processing/claims\"),\n",
    "        ProcessingInput(source=customers_data_uri, destination=\"/opt/ml/processing/customers\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/train_data\", output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/validation_data\", output_name=\"validation_data\", source=\"/opt/ml/processing/val\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/test_data\", output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/processed_data\", output_name=\"processed_data\", source=\"/opt/ml/processing/full\")\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--train-ratio\", \"0.8\", \n",
    "        \"--validation-ratio\", \"0.1\",\n",
    "        \"--test-ratio\", \"0.1\"\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/preprocessing.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp-scripts/xgboost_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp-scripts/xgboost_train.py\n",
    "\n",
    "# Cell 08\n",
    "import argparse\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters and algorithm parameters are described here\n",
    "    parser.add_argument(\"--num_round\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--eta\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--colsample_bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--objective\", type=str, default=\"binary:logistic\")\n",
    "    parser.add_argument(\"--eval_metric\", type=str, default=\"auc\")\n",
    "    parser.add_argument(\"--nfold\", type=int, default=3)\n",
    "    parser.add_argument(\"--early_stopping_rounds\", type=int, default=3)\n",
    "    \n",
    "\n",
    "    # SageMaker specific arguments. Defaults are set in the environment variables\n",
    "    # Set location of input training data\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    # Set location of input validation data\n",
    "    parser.add_argument(\"--validation_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    # Set location where trained model will be stored. Default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    # Set location where model artifacts will be stored. Default set by SageMaker, /opt/ml/output/data\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_train = pd.read_csv(f\"{args.train_data_dir}/train.csv\")\n",
    "    train = data_train.drop(\"fraud\", axis=1)\n",
    "    label_train = pd.DataFrame(data_train[\"fraud\"])\n",
    "    dtrain = xgb.DMatrix(train, label=label_train)\n",
    "    \n",
    "    \n",
    "    data_validation = pd.read_csv(f\"{args.validation_data_dir}/validation.csv\")\n",
    "    validation = data_validation.drop(\"fraud\", axis=1)\n",
    "    label_validation = pd.DataFrame(data_validation[\"fraud\"])\n",
    "    dvalidation = xgb.DMatrix(validation, label=label_validation)\n",
    "    \n",
    "    # Choose XGBoost model hyperparameters\n",
    "    params = {\"max_depth\": args.max_depth,\n",
    "              \"eta\": args.eta,\n",
    "              \"objective\": args.objective,\n",
    "              \"subsample\" : args.subsample,\n",
    "              \"colsample_bytree\":args.colsample_bytree\n",
    "             }\n",
    "    \n",
    "    num_boost_round = args.num_round\n",
    "    nfold = args.nfold\n",
    "    early_stopping_rounds = args.early_stopping_rounds\n",
    "    \n",
    "    # Cross-validate train XGBoost model\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        metrics=[\"auc\"],\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=len(cv_results))\n",
    "    \n",
    "    train_pred = model.predict(dtrain)\n",
    "    validation_pred = model.predict(dvalidation)\n",
    "    \n",
    "    train_auc = roc_auc_score(label_train, train_pred)\n",
    "    validation_auc = roc_auc_score(label_validation, validation_pred)\n",
    "    \n",
    "    print(f\"[0]#011train-auc:{train_auc:.2f}\")\n",
    "    print(f\"[0]#011validation-auc:{validation_auc:.2f}\")\n",
    "\n",
    "    metrics_data = {\"hyperparameters\" : params,\n",
    "                    \"binary_classification_metrics\": {\"validation:auc\": {\"value\": validation_auc},\n",
    "                                                      \"train:auc\": {\"value\": train_auc}\n",
    "                                                     }\n",
    "                   }\n",
    "              \n",
    "    # Save the evaluation metrics to the location specified by output_data_dir\n",
    "    metrics_location = args.output_data_dir + \"/metrics.json\"\n",
    "    \n",
    "    # Save the trained model to the location specified by model_dir\n",
    "    model_location = args.model_dir + \"/xgboost-model\"\n",
    "\n",
    "    with open(metrics_location, \"w\") as f:\n",
    "        json.dump(metrics_data, f)\n",
    "\n",
    "    with open(model_location, \"wb\") as f:\n",
    "        joblib.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 08 \n",
    "\n",
    "# Set XGBoost model hyperparameters \n",
    "hyperparams = {  \n",
    "    \"eval_metric\" : \"auc\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"5\",\n",
    "    \"max_depth\":\"5\",\n",
    "    \"subsample\":\"0.75\",\n",
    "    \"colsample_bytree\":\"0.75\",\n",
    "    \"eta\":\"0.5\"\n",
    "}\n",
    "\n",
    "# Set XGBoost estimator\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"./temp-scripts/xgboost_train.py\", \n",
    "    output_path=estimator_output_uri,\n",
    "    code_location=estimator_output_uri,\n",
    "    hyperparameters=hyperparams,\n",
    "    role=sagemaker_role,\n",
    "    # Fetch instance type and count from pipeline parameters\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"1.3-1\"\n",
    ")\n",
    "\n",
    "# Access the location where the preceding processing step saved train and validation datasets\n",
    "# Pipeline step properties can give access to outputs which can be used in succeeding steps\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri, \n",
    "    content_type=\"csv\", \n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\"validation_data\"].S3Output.S3Uri,\n",
    "    content_type=\"csv\",\n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "\n",
    "# Set pipeline training step\n",
    "train_step = TrainingStep(\n",
    "    name=\"XGBModelTraining\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        \"train\":s3_input_train, # Train channel \n",
    "        \"validation\": s3_input_validation # Validation channel\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 09\n",
    "# Create a SageMaker model\n",
    "model = sagemaker.model.Model(\n",
    "    image_uri=training_image,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "# Specify model deployment instance type\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=deploy_instance_type_param)\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"FraudDetModel\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Set up common configuration parameters to be used across multiple steps\n",
    "check_job_config = CheckJobConfig(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=clarify_instance_type,\n",
    "    volume_size_in_gb=30,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Set up configuration of data to be used for model bias check\n",
    "model_bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    # Fetch S3 location where processing step saved train data\n",
    "    s3_data_input_path=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "    s3_output_path=bias_report_output_uri,\n",
    "    label=target_col,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=clarify_bias_config_output_uri\n",
    ")\n",
    "\n",
    "# Set up details of the trained model to be checked for bias\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    # Pull model name from model creation step\n",
    "    model_name=create_model_step.properties.ModelName,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type\n",
    ")\n",
    "\n",
    "# Set up column and categories that are to be checked for bias\n",
    "model_bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name=\"customer_gender_female\",\n",
    "    facet_values_or_threshold=[1]\n",
    ")\n",
    "\n",
    "# Set up model predictions configuration to get binary labels from probabilities\n",
    "model_predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)\n",
    "\n",
    "model_bias_check_config = ModelBiasCheckConfig(\n",
    "    data_config=model_bias_data_config,\n",
    "    data_bias_config=model_bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=model_predictions_config,\n",
    "    methods=[\"DPPL\"]\n",
    ")\n",
    "\n",
    "# Set up pipeline model bias check step\n",
    "model_bias_check_step = ClarifyCheckStep(\n",
    "    name=\"ModelBiasCheck\",\n",
    "    clarify_check_config=model_bias_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_bias_param,\n",
    "    register_new_baseline=register_new_baseline_model_bias_param,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_bias_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# Set configuration of data to be used for model explainability check\n",
    "model_explainability_data_config = sagemaker.clarify.DataConfig(\n",
    "    # Fetch S3 location where processing step saved train data\n",
    "    s3_data_input_path=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "    s3_output_path=explainability_report_output_uri,\n",
    "    label=target_col,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=clarify_explainability_config_output_uri \n",
    ")\n",
    "\n",
    "# Set SHAP configuration for Clarify to compute global and local SHAP values for feature importance\n",
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    seed=42, \n",
    "    num_samples=100,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True\n",
    ")\n",
    "\n",
    "model_explainability_config = ModelExplainabilityCheckConfig(\n",
    "    data_config=model_explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config\n",
    ")\n",
    "\n",
    "# Set pipeline model explainability check step\n",
    "model_explainability_step = ClarifyCheckStep(\n",
    "    name=\"ModelExplainabilityCheck\",\n",
    "    clarify_check_config=model_explainability_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_explainability_param,\n",
    "    register_new_baseline=register_new_baseline_model_explainability_param,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_explainability_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp-scripts/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp-scripts/evaluate.py\n",
    "\n",
    "# Cell 12\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    # The name of the file should match how the model was saved in the training script\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    test_local_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df_test = pd.read_csv(test_local_path)\n",
    "    \n",
    "    # Extract test set target column\n",
    "    y_test = df_test.iloc[:, 0].values\n",
    "   \n",
    "    cols_when_train = model.feature_names\n",
    "    # Extract test set feature columns\n",
    "    X = df_test[cols_when_train].copy()\n",
    "    X_test = xgb.DMatrix(X)\n",
    "\n",
    "    logger.info(\"Generating predictions for test data.\")\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate model evaluation score\n",
    "    logger.debug(\"Calculating ROC-AUC score.\")\n",
    "    auc = roc_auc_score(y_test, pred)\n",
    "    metric_dict = {\n",
    "        \"classification_metrics\": {\"roc_auc\": {\"value\": auc}}\n",
    "    }\n",
    "    \n",
    "    # Save model evaluation metrics\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Writing evaluation report with ROC-AUC: %f\", auc)\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(metric_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Upload model evaluation script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"temp-scripts/evaluate.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/evaluate.py\"\n",
    ")\n",
    "\n",
    "eval_processor = ScriptProcessor(\n",
    "    image_uri=training_image,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=predictor_instance_type,\n",
    "    instance_count=predictor_instance_count,\n",
    "    base_job_name=f\"{base_job_name_prefix}-model-eval\",\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"FraudDetEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "# Set model evaluation step\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"XGBModelEvaluate\",\n",
    "    processor=eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where train step saved model artifacts\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where processing step saved test data\n",
    "            source=process_step.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{model_eval_output_uri}\", output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# Fetch baseline constraints to record in model registry\n",
    "model_metrics = ModelMetrics(\n",
    "    bias_post_training=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\"\n",
    "    ),\n",
    "    explainability=MetricsSource(\n",
    "        s3_uri=model_explainability_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch baselines to record in model registry for drift check\n",
    "drift_check_baselines = DriftCheckBaselines(\n",
    "    bias_post_training_constraints=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_constraints=MetricsSource(\n",
    "        s3_uri=model_explainability_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_config_file=FileSource(\n",
    "        s3_uri=model_explainability_config.monitoring_analysis_config_uri,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define register model step\n",
    "register_step = RegisterModel(\n",
    "    name=\"XGBRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    # Fetching S3 location where train step saved model artifacts\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[predictor_instance_type],\n",
    "    transform_instances=[predictor_instance_type],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status_param,\n",
    "    # Registering baselines metrics that can be used for model monitoring\n",
    "    model_metrics=model_metrics,\n",
    "    drift_check_baselines=drift_check_baselines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp-scripts/lambda_deployer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp-scripts/lambda_deployer.py\n",
    "\n",
    "# Cell 15\n",
    "\"\"\"\n",
    "Lambda function creates an endpoint configuration and deploys a model to real-time endpoint. \n",
    "Required parameters for deployment are retrieved from the event object\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # Details of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    instance_type = event[\"instance_type\"]\n",
    "    instance_count = event[\"instance_count\"]\n",
    "    primary_container = {\"ModelPackageName\": model_package_arn}\n",
    "\n",
    "    # Create model\n",
    "    model = sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer=primary_container,\n",
    "        ExecutionRoleArn=role\n",
    "    )\n",
    "\n",
    "    # Create endpoint configuration\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"Alltraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": instance_count,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create endpoint\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "# The function name must contain sagemaker\n",
    "function_name = \"sagemaker-fraud-det-demo-lambda-step\"\n",
    "# Define Lambda helper class can be used to create the Lambda function required in the Lambda step\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=sagemaker_role,\n",
    "    script=\"./temp-scripts/lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The inputs used in the lambda handler are passed through the inputs argument in the \n",
    "# LambdaStep and retrieved via the `event` object within the `lambda_handler` function\n",
    "\n",
    "lambda_deploy_step = LambdaStep(\n",
    "    name=\"LambdaStepRealTimeDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": pipeline_model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": register_step.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": sagemaker_role,\n",
    "        \"instance_type\": deploy_instance_type_param,\n",
    "        \"instance_count\": deploy_instance_count_param\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "# Evaluate model performance on test set\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.roc_auc.value\",\n",
    "    ),\n",
    "    right=0.7, # Threshold to compare model performance against\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckFraudDetXGBEvaluation\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[create_model_step, model_bias_check_step, model_explainability_step, register_step], #, lambda_deploy_step], \n",
    "    else_steps=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "# Create the Pipeline with all component steps and parameters\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[process_instance_type_param, \n",
    "                train_instance_type_param, \n",
    "                train_instance_count_param, \n",
    "                deploy_instance_type_param,\n",
    "                deploy_instance_count_param,\n",
    "                clarify_instance_type_param,\n",
    "                skip_check_model_bias_param,\n",
    "                register_new_baseline_model_bias_param,\n",
    "                supplied_baseline_constraints_model_bias_param,\n",
    "                skip_check_model_explainability_param,\n",
    "                register_new_baseline_model_explainability_param,\n",
    "                supplied_baseline_constraints_model_explainability_param,\n",
    "                model_approval_status_param],\n",
    "    steps=[\n",
    "        process_step,\n",
    "        train_step,\n",
    "        evaluation_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    sagemaker_session=sess\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "# Full Pipeline description\n",
    "pipeline_definition = json.loads(pipeline.describe()['PipelineDefinition'])\n",
    "pipeline_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell # 20\n",
    "# Execute Pipeline\n",
    "start_response = pipeline.start(parameters=dict(\n",
    "        SkipModelBiasCheck=True,\n",
    "        RegisterNewModelBiasBaseline=True,\n",
    "        SkipModelExplainabilityCheck=True,\n",
    "        RegisterNewModelExplainabilityBaseline=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run only\n",
    "Only run this if you see your Pipeline in success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21\n",
    "# Fetch test data to run predictions with the endpoint\n",
    "test_df = pd.read_csv(f\"{processing_output_uri}/test_data/test.csv\")\n",
    "\n",
    "# Create SageMaker Predictor from the deployed endpoint\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, \n",
    "                                          sagemaker_session=sess,\n",
    "                                          serializer=CSVSerializer(),\n",
    "                                          deserializer=CSVDeserializer()\n",
    "                                         )\n",
    "# Test endpoint with payload of 5 samples\n",
    "payload = test_df.drop([\"fraud\"], axis=1).iloc[:5]\n",
    "result = predictor.predict(payload.values)\n",
    "prediction_df = pd.DataFrame()\n",
    "prediction_df[\"Prediction\"] = result\n",
    "prediction_df[\"Label\"] = test_df[\"fraud\"].iloc[:5].values\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "Clean up the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "# Delete the Lambda function\n",
    "func.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete the EndpointConfig\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete the model\n",
    "sm_client.delete_model(ModelName=pipeline_model_name)\n",
    "\n",
    "# Delete the pipeline\n",
    "sm_client.delete_pipeline(PipelineName=pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
